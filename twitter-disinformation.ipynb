{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iranian State-Sponsored Disinformation Campaigns on Twitter \n",
    "### Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data cleaning\n",
    "\n",
    "#### a). Load the raw Twitter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "# Load each file in the data folder and add it to the same file\n",
    "def load_twitter_data():\n",
    "    \"\"\"\n",
    "    A function to load scraped news data from data folder\n",
    "    \"\"\"\n",
    "    # List of files\n",
    "    files = [f for f in os.listdir(os.path.join(os.getcwd(), \"src_data\")) if f.endswith(\".csv\")]\n",
    "    \n",
    "    # List of data frames\n",
    "    file_list = []\n",
    "    \n",
    "    # Append each data frame in files to the file_list\n",
    "    for filename in files:\n",
    "        df = pd.read_csv(os.path.join(os.path.join(os.getcwd(), \"src_data\"), filename))\n",
    "        file_list.append(df)\n",
    "        \n",
    "    # Concatenate all the news data frames\n",
    "    df_full = pd.concat(file_list, join='outer').drop_duplicates().reset_index().drop(columns='index')\n",
    "    \n",
    "    return df_full\n",
    "\n",
    "tweets = load_twitter_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b). Select only the variables we are interestedted in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_screen_name</th>\n",
       "      <th>user_display_name</th>\n",
       "      <th>user_reported_location</th>\n",
       "      <th>account_language</th>\n",
       "      <th>tweet_language</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>tweet_time</th>\n",
       "      <th>urls</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>is_retweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>akhonfellah</td>\n",
       "      <td>⁦🇮🇷⁩أخٌ‌في‌الله</td>\n",
       "      <td>Iran</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>one person followed me // automatically checke...</td>\n",
       "      <td>2017-01-11 05:23</td>\n",
       "      <td>['http://fllwrs.com']</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>akhonfellah</td>\n",
       "      <td>⁦🇮🇷⁩أخٌ‌في‌الله</td>\n",
       "      <td>Iran</td>\n",
       "      <td>en</td>\n",
       "      <td>fa</td>\n",
       "      <td>#IDFTerrorists\\nحماسه تروریستهای اسرائیلی http...</td>\n",
       "      <td>2018-05-26 00:48</td>\n",
       "      <td>[]</td>\n",
       "      <td>['IDFTerrorists']</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>akhonfellah</td>\n",
       "      <td>⁦🇮🇷⁩أخٌ‌في‌الله</td>\n",
       "      <td>Iran</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>Stop war on Yemen hospitals\\n#ShameOnUN\\n#Yemen</td>\n",
       "      <td>2018-06-16 20:06</td>\n",
       "      <td>[]</td>\n",
       "      <td>['ShameOnUN', 'Yemen']</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>akhonfellah</td>\n",
       "      <td>⁦🇮🇷⁩أخٌ‌في‌الله</td>\n",
       "      <td>Iran</td>\n",
       "      <td>en</td>\n",
       "      <td>fa</td>\n",
       "      <td>لبیک یا فقیه\\n#مجزرة_الدراز https://t.co/nKfQW...</td>\n",
       "      <td>2018-05-23 18:22</td>\n",
       "      <td>[]</td>\n",
       "      <td>['مجزرة_الدراز']</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>akhonfellah</td>\n",
       "      <td>⁦🇮🇷⁩أخٌ‌في‌الله</td>\n",
       "      <td>Iran</td>\n",
       "      <td>en</td>\n",
       "      <td>fa</td>\n",
       "      <td>اینجا تل ابیب است\\nاینها اسراییلیهایی هستند که...</td>\n",
       "      <td>2019-01-28 16:56</td>\n",
       "      <td>[]</td>\n",
       "      <td>['زندگی_سگی_اسرائیلیها']</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>akhonfellah</td>\n",
       "      <td>⁦🇮🇷⁩أخٌ‌في‌الله</td>\n",
       "      <td>Iran</td>\n",
       "      <td>en</td>\n",
       "      <td>ar</td>\n",
       "      <td>وامروز هم ....\\n#زندگی_سگی_اسرائیلیها https://...</td>\n",
       "      <td>2018-09-07 10:42</td>\n",
       "      <td>[]</td>\n",
       "      <td>['زندگی_سگی_اسرائیلیها']</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>akhonfellah</td>\n",
       "      <td>⁦🇮🇷⁩أخٌ‌في‌الله</td>\n",
       "      <td>Iran</td>\n",
       "      <td>en</td>\n",
       "      <td>fa</td>\n",
       "      <td>حکمت ثابت موندن اسم ماههای قمری بعد از تغییر ز...</td>\n",
       "      <td>2017-11-19 19:40</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>akhonfellah</td>\n",
       "      <td>⁦🇮🇷⁩أخٌ‌في‌الله</td>\n",
       "      <td>Iran</td>\n",
       "      <td>en</td>\n",
       "      <td>fa</td>\n",
       "      <td>جمله ای که سیدحسن امشب گفت در مورد انقلاب اسلا...</td>\n",
       "      <td>2019-02-06 18:26</td>\n",
       "      <td>[]</td>\n",
       "      <td>['إن_مع_الصبر_نصرا']</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>akhonfellah</td>\n",
       "      <td>⁦🇮🇷⁩أخٌ‌في‌الله</td>\n",
       "      <td>Iran</td>\n",
       "      <td>en</td>\n",
       "      <td>fa</td>\n",
       "      <td>بشنوید مدح حاج محمودآقوی کریمی رو با لهجه شیرا...</td>\n",
       "      <td>2017-08-04 12:25</td>\n",
       "      <td>['https://twitter.com/khanisadiq/status/893438...</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>akhonfellah</td>\n",
       "      <td>⁦🇮🇷⁩أخٌ‌في‌الله</td>\n",
       "      <td>Iran</td>\n",
       "      <td>en</td>\n",
       "      <td>ar</td>\n",
       "      <td>RT @awadazeinab1: كم ساعة مع ابني بالمستشفى شف...</td>\n",
       "      <td>2019-01-25 17:48</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_screen_name user_display_name user_reported_location account_language  \\\n",
       "0      akhonfellah   ⁦🇮🇷⁩أخٌ‌في‌الله                   Iran               en   \n",
       "1      akhonfellah   ⁦🇮🇷⁩أخٌ‌في‌الله                   Iran               en   \n",
       "2      akhonfellah   ⁦🇮🇷⁩أخٌ‌في‌الله                   Iran               en   \n",
       "3      akhonfellah   ⁦🇮🇷⁩أخٌ‌في‌الله                   Iran               en   \n",
       "4      akhonfellah   ⁦🇮🇷⁩أخٌ‌في‌الله                   Iran               en   \n",
       "5      akhonfellah   ⁦🇮🇷⁩أخٌ‌في‌الله                   Iran               en   \n",
       "6      akhonfellah   ⁦🇮🇷⁩أخٌ‌في‌الله                   Iran               en   \n",
       "7      akhonfellah   ⁦🇮🇷⁩أخٌ‌في‌الله                   Iran               en   \n",
       "8      akhonfellah   ⁦🇮🇷⁩أخٌ‌في‌الله                   Iran               en   \n",
       "9      akhonfellah   ⁦🇮🇷⁩أخٌ‌في‌الله                   Iran               en   \n",
       "\n",
       "  tweet_language                                         tweet_text  \\\n",
       "0             en  one person followed me // automatically checke...   \n",
       "1             fa  #IDFTerrorists\\nحماسه تروریستهای اسرائیلی http...   \n",
       "2             en    Stop war on Yemen hospitals\\n#ShameOnUN\\n#Yemen   \n",
       "3             fa  لبیک یا فقیه\\n#مجزرة_الدراز https://t.co/nKfQW...   \n",
       "4             fa  اینجا تل ابیب است\\nاینها اسراییلیهایی هستند که...   \n",
       "5             ar  وامروز هم ....\\n#زندگی_سگی_اسرائیلیها https://...   \n",
       "6             fa  حکمت ثابت موندن اسم ماههای قمری بعد از تغییر ز...   \n",
       "7             fa  جمله ای که سیدحسن امشب گفت در مورد انقلاب اسلا...   \n",
       "8             fa  بشنوید مدح حاج محمودآقوی کریمی رو با لهجه شیرا...   \n",
       "9             ar  RT @awadazeinab1: كم ساعة مع ابني بالمستشفى شف...   \n",
       "\n",
       "         tweet_time                                               urls  \\\n",
       "0  2017-01-11 05:23                              ['http://fllwrs.com']   \n",
       "1  2018-05-26 00:48                                                 []   \n",
       "2  2018-06-16 20:06                                                 []   \n",
       "3  2018-05-23 18:22                                                 []   \n",
       "4  2019-01-28 16:56                                                 []   \n",
       "5  2018-09-07 10:42                                                 []   \n",
       "6  2017-11-19 19:40                                                 []   \n",
       "7  2019-02-06 18:26                                                 []   \n",
       "8  2017-08-04 12:25  ['https://twitter.com/khanisadiq/status/893438...   \n",
       "9  2019-01-25 17:48                                                 []   \n",
       "\n",
       "                   hashtags  is_retweet  \n",
       "0                        []       False  \n",
       "1         ['IDFTerrorists']       False  \n",
       "2    ['ShameOnUN', 'Yemen']       False  \n",
       "3          ['مجزرة_الدراز']       False  \n",
       "4  ['زندگی_سگی_اسرائیلیها']       False  \n",
       "5  ['زندگی_سگی_اسرائیلیها']       False  \n",
       "6                        []       False  \n",
       "7      ['إن_مع_الصبر_نصرا']       False  \n",
       "8                        []       False  \n",
       "9                        []        True  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_clean = tweets[['user_screen_name',  'user_display_name', 'user_reported_location', 'account_language', 'tweet_language', 'tweet_text', 'tweet_time', 'urls', 'hashtags', 'is_retweet']]\n",
    "\n",
    "tweets_clean.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c). Filter tweets and keep those that are\n",
    "- account location in Venezuela\n",
    "- account language is Spanish or\n",
    "- Tweet language is Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_clean = tweets_clean[(tweets_clean.user_reported_location == 'Venezuela') | (tweets_clean.account_language == 'es') | (tweets_clean.tweet_language == 'es')].reset_index().drop(columns= ['index'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d.) Take out tweets that are set in European and US locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_clean = tweets_clean[(tweets_clean.user_reported_location != 'London') & (tweets_clean.user_reported_location != 'Manhattan, NY') & (tweets_clean.user_reported_location != 'Brooklyn, NY') & (tweets_clean.user_reported_location != 'Queens, NY') & (tweets_clean.user_reported_location != 'New York, NY') & (tweets_clean.user_reported_location != 'California, USA') & (tweets_clean.user_reported_location != 'New Jersey, USA') &  (tweets_clean.user_reported_location != 'North Holland, The Netherlands') & (tweets_clean.user_reported_location != 'Atlantic City, NJ') & (tweets_clean.user_reported_location != 'Mountain View, CA') & (tweets_clean.user_reported_location != 'New York, USA') & (tweets_clean.user_reported_location != 'Canada') & (tweets_clean.user_reported_location != 'San Francisco, CA') & (tweets_clean.user_reported_location != 'Washington, USA') & (tweets_clean.user_reported_location != 'Washington, DC') & (tweets_clean.user_reported_location != 'España') & (tweets_clean.user_reported_location != 'Germany') & (tweets_clean.user_reported_location != 'Nantes, France') & (tweets_clean.user_reported_location != 'Houston, TX') & (tweets_clean.user_reported_location != 'Texas,San Antonio') & (tweets_clean.user_reported_location != 'Chicago') & (tweets_clean.user_reported_location != 'Atlanta') & (tweets_clean.user_reported_location != 'Washington,Seattle') & (tweets_clean.user_reported_location != 'Fremont, CA') & (tweets_clean.user_reported_location != 'France') & (tweets_clean.user_reported_location != 'England, United Kingdom')  & (tweets_clean.user_reported_location != 'Oregon,Portland')  & (tweets_clean.user_reported_location !='USA')  & (tweets_clean.user_reported_location != 'Florida,Orlando') & (tweets_clean.user_reported_location != 'Califor') & (tweets_clean.user_reported_location !='California,Los Angeles') & (tweets_clean.user_reported_location !='Illinois, USA') & (tweets_clean.user_reported_location !='Arizona,phoenix') & (tweets_clean.user_reported_location !='Pennsylvania,Pittsburgh') & (tweets_clean.user_reported_location !='Pennsylvania,Philadelphia') & (tweets_clean.user_reported_location !='Dallas, TX') ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e.) Quality control for Tweet Language\n",
    "\n",
    "- Use SpaCy language identification to cross-check Twitter’s language identification. Identify language of each tweet using SpaCy.\n",
    "- Exclude tweets on which SpaCy and Twitter don't agree on the language\n",
    "- Spanish and English datasets consist of Tweets that were marked as either Spanish by BOTH Twitter and SpaCy or English by BOTH Twitter and SpaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy_langdetect import LanguageDetector\n",
    "\n",
    "\n",
    "# Initialize spacy with the SPANISH model\n",
    "sp = spacy.load('es_core_news_sm')\n",
    "sp.add_pipe(LanguageDetector(), name = 'language_detector', last = True)\n",
    "eng = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_language(df, content_column):\n",
    "    '''\n",
    "    A function to detect the language in each tweet and add to new row\n",
    "\n",
    "    Argument: a dataframe  and content column\n",
    "    Ouput: same dataframe with a new 'cleaned_content' column\n",
    "    '''\n",
    "\n",
    "    # Initialize list of languages\n",
    "    spacy_language_detection = []\n",
    "\n",
    "    # Call detect the language for each row in the data frame and append to spacy_language_detection list\n",
    "    for row in df[content_column]:\n",
    "        doc = sp(row)\n",
    "        spacy_language_detection.append(doc._.language['language'])\n",
    "\n",
    "    # Append language list to the data frame\n",
    "    df['spacy_language_detection'] = spacy_language_detection\n",
    "\n",
    "    return df \n",
    "\n",
    "tweets_clean = detect_language(df = tweets_clean, content_column = 'tweet_text')\n",
    "\n",
    "# Isolate tweets marked as Spanish by Twitter AND SpaCy \n",
    "spanish_tweets = tweets_clean[(tweets_clean.tweet_language == 'es') & (tweets_clean.spacy_language_detection == 'es')]\n",
    "\n",
    "\n",
    "# Isolate tweets marked as English by Twitter AND SpaCy \n",
    "english_tweets = tweets_clean[(tweets_clean.tweet_language == 'en') & (tweets_clean.spacy_language_detection == 'en')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_tweets = pd.read_csv('proj_data/spanish_tweets.csv', encoding='utf-8-sig', index_col = 0).reset_index()\n",
    "english_tweets = pd.read_csv('proj_data/english_tweets.csv', encoding='utf-8-sig', index_col = 0).reset_index()\n",
    "english_tweets.lemmatized_tweet_text = english_tweets.lemmatized_tweet_text.astype('str')\n",
    "spanish_tweets.lemmatized_tweet_text = spanish_tweets.lemmatized_tweet_text.astype('str')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Keyword analysis\n",
    "\n",
    "### a.) Lemmatize and simplify Tweet text\n",
    "- “Clean” each Tweet by removing stop words (very common words that carry little meaning), non alpha-numeric characters, and by reducing each word to it’s lemma or root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(text_string, language):\n",
    "    '''\n",
    "    A function to clean a string using SpaCy, removing stop-words and non-alphanumeric characters\n",
    "\n",
    "    Argument: a text string and a language ('English' or 'Spanish')\n",
    "    Output: a cleaned string\n",
    "\n",
    "    '''\n",
    "    if language == 'Spanish':\n",
    "    # Parse the text string using the english model initialized earlier\n",
    "        doc = sp(text_string)\n",
    "    elif language == 'English':\n",
    "        doc = eng(text_string)\n",
    "    \n",
    "    # Initialize empty string\n",
    "    clean = []\n",
    "\n",
    "    # Add each token to the list if it is not a stop word, is alphanumeric, and if it's not a pronoun\n",
    "    for token in doc:\n",
    "        \n",
    "        if token.is_alpha == False or token.is_stop == True:\n",
    "            pass\n",
    "        else:\n",
    "            clean.append(token.lemma_)\n",
    "\n",
    "    # Join the list into a string\n",
    "    clean = \" \".join(clean)\n",
    "\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example of how the cleaning works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw example: \n",
      "RT @Algbarow55: Solo en #Yemen los padres pierden a sus hijos; Ni√±os pierden a sus padres #SaudiBombsChildren https://t.co/uImWl8QG5u\n",
      "\n",
      "\n",
      "Clean exmaple: \n",
      "RT Solo Yemen padre perder a hijo perder a padre SaudiBombsChildren\n"
     ]
    }
   ],
   "source": [
    "example_sp = spanish_tweets.loc[2,'tweet_text']\n",
    "example_sp_clean = clean_string(example_sp, \"Spanish\")\n",
    "print(\"Raw example: \\n\" + example_sp)\n",
    "print(\"\\n\\nClean exmaple: \\n\" + example_sp_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_content(df, content_column, language):\n",
    "    '''\n",
    "    A function to clean all the strings in a whole of a corpus\n",
    "\n",
    "    Argument: a dataframe, the name of the column with the content, and a language ('Spanish' or 'English')\n",
    "    Ouput: same dataframe with a new cleaned content column\n",
    "    '''\n",
    "\n",
    "    # Initialize list of cleaned content strings\n",
    "    clean_content= []\n",
    "\n",
    "    # Call clean_string() for each row in the data frame and append to clean_content list\n",
    "    for row in df[content_column]:\n",
    "        clean_content.append(clean_string(row, language))\n",
    "\n",
    "    # Append clean_content list to the data frame\n",
    "    df['lemmatized_tweet_text'] = clean_content\n",
    "\n",
    "    return df \n",
    "\n",
    "spanish_tweets = clean_content(spanish_tweets, 'tweet_text', 'Spanish')\n",
    "\n",
    "english_tweets = clean_content(english_tweets, 'tweet_text', 'English')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b.) Create word counts\n",
    "\n",
    "Use Scikit-Learn CountVectorizer create a matrix where each column is a word found in any Tweet and each row is the number of times it occurs in each Tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only words that appear in more than 5 tweets (a way to decrease the size of the vocab)\n",
    "word_vectorizer = CountVectorizer(encoding='utf-8-sig', analyzer='word', min_df=5, ngram_range=(1,1))\n",
    "# create matrix where each column is a word and each row is a count in each tweet\n",
    "word_count_sm = word_vectorizer.fit_transform(spanish_tweets['lemmatized_tweet_text'])\n",
    "words = word_vectorizer.get_feature_names()\n",
    "\n",
    "# sum the count of each word over all Tweets\n",
    "word_count_total = word_count_sm.sum(axis=0)\n",
    "word_count_total_df = pd.DataFrame(word_count_total, columns = words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abadi</th>\n",
       "      <th>abajar</th>\n",
       "      <th>abajo</th>\n",
       "      <th>abandonar</th>\n",
       "      <th>abatir</th>\n",
       "      <th>abbas</th>\n",
       "      <th>abby</th>\n",
       "      <th>abc</th>\n",
       "      <th>abdel</th>\n",
       "      <th>abdullah</th>\n",
       "      <th>...</th>\n",
       "      <th>äúpor</th>\n",
       "      <th>äúsegundo</th>\n",
       "      <th>äúsionistas</th>\n",
       "      <th>äútsunami</th>\n",
       "      <th>äúun</th>\n",
       "      <th>äúyo</th>\n",
       "      <th>òë</th>\n",
       "      <th>òï</th>\n",
       "      <th>ùå</th>\n",
       "      <th>üí</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>8</td>\n",
       "      <td>79</td>\n",
       "      <td>50</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>26</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 6750 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abadi  abajar  abajo  abandonar  abatir  abbas  abby  abc  abdel  abdullah  \\\n",
       "0     18      19      8         79      50     15     6    5     17        10   \n",
       "\n",
       "   ...  äúpor  äúsegundo  äúsionistas  äútsunami  äúun  äúyo  òë  òï  ùå  üí  \n",
       "0  ...      5          6            6         18    15    11   6  13  26  14  \n",
       "\n",
       "[1 rows x 6750 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count_total_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c.) Phrase counts\n",
    "\n",
    "Change the number of words analyzed with the same type of vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only phrases that appear in more than 5 tweets (a way to decrease the size of the vocab)\n",
    "phrase_vectorizer = CountVectorizer(encoding='utf-8-sig', analyzer='word', min_df=5, ngram_range=(5,7))\n",
    "# create matrix where each column is a phrase and each row is a count in each tweet\n",
    "phrase_count_sm = phrase_vectorizer.fit_transform(spanish_tweets['lemmatized_tweet_text'])\n",
    "phrases = phrase_vectorizer.get_feature_names()\n",
    "# sum the count of each phrase over all Tweets\n",
    "phrase_count_total = phrase_count_sm.sum(axis=0)\n",
    "phrase_count_total_df = pd.DataFrame(phrase_count_total, columns = phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abby martin exponer fascista colonial</th>\n",
       "      <th>abby martin exponer fascista colonial israel</th>\n",
       "      <th>abby martin exponer fascista colonial israel sionismo</th>\n",
       "      <th>abdel rahman hussein abu hmash</th>\n",
       "      <th>abdel rahman hussein abu hmash palestino</th>\n",
       "      <th>abdel rahman hussein abu hmash palestino asesinar</th>\n",
       "      <th>abogado hermann el pdte llamar</th>\n",
       "      <th>abogado hermann el pdte llamar reflexionar</th>\n",
       "      <th>abogado hermann el pdte llamar reflexionar pensar</th>\n",
       "      <th>abogados casar narvarte objetivo malware</th>\n",
       "      <th>...</th>\n",
       "      <th>äúes fuerza seguridad estado entrar</th>\n",
       "      <th>äúes fuerza seguridad estado entrar perseguir</th>\n",
       "      <th>äúgobernadores adversar gobierno renunciar resignarse</th>\n",
       "      <th>äúgobernadores adversar gobierno renunciar resignarse titular</th>\n",
       "      <th>äúno tarea segundo importante general</th>\n",
       "      <th>äúno tarea segundo importante general promover</th>\n",
       "      <th>äúno tarea segundo importante general promover proteger</th>\n",
       "      <th>äúyo desear segundo formar rico</th>\n",
       "      <th>äúyo desear segundo formar rico segundo</th>\n",
       "      <th>äúyo desear segundo formar rico segundo grande</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 8746 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abby martin exponer fascista colonial  \\\n",
       "0                                      6   \n",
       "\n",
       "   abby martin exponer fascista colonial israel  \\\n",
       "0                                             6   \n",
       "\n",
       "   abby martin exponer fascista colonial israel sionismo  \\\n",
       "0                                                  6       \n",
       "\n",
       "   abdel rahman hussein abu hmash  abdel rahman hussein abu hmash palestino  \\\n",
       "0                              10                                        10   \n",
       "\n",
       "   abdel rahman hussein abu hmash palestino asesinar  \\\n",
       "0                                                 10   \n",
       "\n",
       "   abogado hermann el pdte llamar  abogado hermann el pdte llamar reflexionar  \\\n",
       "0                              14                                          14   \n",
       "\n",
       "   abogado hermann el pdte llamar reflexionar pensar  \\\n",
       "0                                                 14   \n",
       "\n",
       "   abogados casar narvarte objetivo malware  ...  \\\n",
       "0                                         5  ...   \n",
       "\n",
       "   äúes fuerza seguridad estado entrar  \\\n",
       "0                                    6   \n",
       "\n",
       "   äúes fuerza seguridad estado entrar perseguir  \\\n",
       "0                                              6   \n",
       "\n",
       "   äúgobernadores adversar gobierno renunciar resignarse  \\\n",
       "0                                                  7       \n",
       "\n",
       "   äúgobernadores adversar gobierno renunciar resignarse titular  \\\n",
       "0                                                  7               \n",
       "\n",
       "   äúno tarea segundo importante general  \\\n",
       "0                                      8   \n",
       "\n",
       "   äúno tarea segundo importante general promover  \\\n",
       "0                                               8   \n",
       "\n",
       "   äúno tarea segundo importante general promover proteger  \\\n",
       "0                                                  8         \n",
       "\n",
       "   äúyo desear segundo formar rico  äúyo desear segundo formar rico segundo  \\\n",
       "0                                6                                        6   \n",
       "\n",
       "   äúyo desear segundo formar rico segundo grande  \n",
       "0                                               6  \n",
       "\n",
       "[1 rows x 8746 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase_count_total_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d.) Keyword analysis among Tweets that mention Venezuela or Venezolano/a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate Tweets that mention Venezuela or Venezolano/a\n",
    "spanish_tweets_venezuela = spanish_tweets[spanish_tweets.tweet_text.str.contains(\"venez\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Hashtag Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove parens and filter hashtags\n",
    "hashtags = []\n",
    "for hashtag in spanish_tweets.hashtags:\n",
    "    if type(hashtag) == float:\n",
    "        hashtags.append('None')\n",
    "    elif len(hashtag) < 3:\n",
    "        hashtags.append('None')\n",
    "    else:\n",
    "        hashtags.append(hashtag[1:-1])\n",
    "        \n",
    "# Add filtered hashtags back to data frame\n",
    "spanish_tweets.hashtags = hashtags\n",
    "\n",
    "# Spanish hashtag count and save \n",
    "hashtag_vectorizer = CountVectorizer(encoding='utf-8-sig', analyzer='word', ngram_range=(1,1))\n",
    "hashtag_count_sm = hashtag_vectorizer.fit_transform(spanish_tweets['hashtags'])\n",
    "hashtags = hashtag_vectorizer.get_feature_names()\n",
    "hashtag_total = hashtag_count_sm.sum(axis = 0)\n",
    "hashtag_count_df = pd.DataFrame(hashtag_total, columns= hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>01jul</th>\n",
       "      <th>02ago</th>\n",
       "      <th>02feb</th>\n",
       "      <th>03sep</th>\n",
       "      <th>04ago</th>\n",
       "      <th>04nov</th>\n",
       "      <th>06agos</th>\n",
       "      <th>06agosto</th>\n",
       "      <th>09may</th>\n",
       "      <th>09sep</th>\n",
       "      <th>...</th>\n",
       "      <th>ÿäÿü</th>\n",
       "      <th>ÿäÿü_ÿßÿñÿ</th>\n",
       "      <th>ÿæÿ</th>\n",
       "      <th>ÿé_ÿπÿüÿø_ÿßÿµÿøÿçÿßÿäÿé</th>\n",
       "      <th>ÿñÿçÿ</th>\n",
       "      <th>ÿø</th>\n",
       "      <th>ÿøÿäÿ</th>\n",
       "      <th>ÿü</th>\n",
       "      <th>ÿü_ÿ</th>\n",
       "      <th>ÿüÿàÿßÿ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 5476 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   01jul  02ago  02feb  03sep  04ago  04nov  06agos  06agosto  09may  09sep  \\\n",
       "0      2      4      9      8      2      2       2         2      1      5   \n",
       "\n",
       "   ...  ÿäÿü  ÿäÿü_ÿßÿñÿ  ÿæÿ  ÿé_ÿπÿüÿø_ÿßÿµÿøÿçÿßÿäÿé  ÿñÿçÿ  ÿø  ÿøÿäÿ  ÿü  \\\n",
       "0  ...     2           2    4                         2      2   6      2   1   \n",
       "\n",
       "   ÿü_ÿ  ÿüÿàÿßÿ  \n",
       "0     3        1  \n",
       "\n",
       "[1 rows x 5476 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashtag_count_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Link Farming Analysis\n",
    "\n",
    "What proportion of Tweets follow the typical link farming pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_tweets = pd.read_csv('proj_data/spanish_tweets.csv', encoding='utf-8-sig', index_col = 0).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 27677 Tweets with links\n",
      "There are 19215 Tweets without links\n"
     ]
    }
   ],
   "source": [
    "# Replace tweets without link with Null value\n",
    "spanish_tweets.urls = spanish_tweets.urls.replace({'[]': np.nan})\n",
    "\n",
    "\n",
    "tweets_with_link = 0\n",
    "tweets_wout_link = 0\n",
    "\n",
    "for url in spanish_tweets.urls:\n",
    "    if pd.isnull(url) == True:\n",
    "        tweets_with_link +=1\n",
    "    else:\n",
    "        tweets_wout_link += 1\n",
    "        \n",
    "print(\"There are \" + str(tweets_with_link) + \" Tweets with links\")\n",
    "print(\"There are \" + str(tweets_wout_link) + \" Tweets without links\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13858 of the Tweets with links mention other accounts\n"
     ]
    }
   ],
   "source": [
    "spanish_tweets_url  = spanish_tweets[pd.notnull(spanish_tweets.urls)]\n",
    "spanish_tweets_url_at = spanish_tweets_url[spanish_tweets_url.tweet_text.str.contains('@')]\n",
    "print(str(len(spanish_tweets_url_at)) + \" of the Tweets with links mention other accounts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9716 of the Tweets with links are retweets that mention other accounts\n"
     ]
    }
   ],
   "source": [
    "spanish_tweets_url_at_rt = spanish_tweets_url_at[spanish_tweets_url_at.tweet_text.str.startswith('RT')]\n",
    "print(str(len(spanish_tweets_url_at_rt)) + \" of the Tweets with links are retweets that mention other accounts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_tweets_no_content_copy = spanish_tweets.tweet_text.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 46892 total tweets\n"
     ]
    }
   ],
   "source": [
    "print(\"There are \" + str(len(spanish_tweets)) + \" total tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 32014 unique tweets; therefore, 14878 are duplicates.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are \" + str(len(spanish_tweets_no_content_copy)) + \" unique tweets; therefore, \" + str(len(spanish_tweets)- len(spanish_tweets_no_content_copy)) + \" are duplicates.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Date analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_date = []\n",
    "for date in spanish_tweets.tweet_time:\n",
    "    tweet_date.append(str(date)[:-5].strip())\n",
    "\n",
    "spanish_tweets['tweet_date'] = tweet_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_per_day = spanish_tweets.groupby('tweet_date')[['tweet_text']].count()\n",
    "tweets_per_day.to_csv('proj_data/tweets_per_day.csv', encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweet_date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1/1/16</th>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1/1/17</th>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1/1/18</th>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1/10/16</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9/7/17</th>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9/8/16</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9/8/17</th>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9/9/16</th>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9/9/17</th>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1184 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            tweet_text\n",
       "tweet_date            \n",
       "                     2\n",
       "1/1/16              53\n",
       "1/1/17              39\n",
       "1/1/18              68\n",
       "1/10/16              6\n",
       "...                ...\n",
       "9/7/17              99\n",
       "9/8/16               8\n",
       "9/8/17              65\n",
       "9/9/16              18\n",
       "9/9/17              85\n",
       "\n",
       "[1184 rows x 1 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_per_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
