{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iranian State-Sponsored Disinformation Campaigns on Twitter \n",
    "### Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data cleaning\n",
    "\n",
    "#### a). Load the raw Twitter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "# Load each file in the data folder and add it to the same file\n",
    "def load_twitter_data():\n",
    "    \"\"\"\n",
    "    A function to load scraped news data from data folder\n",
    "    \"\"\"\n",
    "    # List of files\n",
    "    files = [f for f in os.listdir(os.path.join(os.getcwd(), \"src_data\")) if f.endswith(\".csv\")]\n",
    "    \n",
    "    # List of data frames\n",
    "    file_list = []\n",
    "    \n",
    "    # Append each data frame in files to the file_list\n",
    "    for filename in files:\n",
    "        df = pd.read_csv(os.path.join(os.path.join(os.getcwd(), \"src_data\"), filename))\n",
    "        file_list.append(df)\n",
    "        \n",
    "    # Concatenate all the news data frames\n",
    "    df_full = pd.concat(file_list, join='outer').drop_duplicates().reset_index().drop(columns='index')\n",
    "    \n",
    "    return df_full\n",
    "\n",
    "tweets = load_twitter_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b). Select only the variables we are interestedted in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_screen_name</th>\n",
       "      <th>user_display_name</th>\n",
       "      <th>user_reported_location</th>\n",
       "      <th>account_language</th>\n",
       "      <th>tweet_language</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>tweet_time</th>\n",
       "      <th>urls</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>is_retweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>akhonfellah</td>\n",
       "      <td>â¦ğŸ‡®ğŸ‡·â©Ø£Ø®ÙŒâ€ŒÙÙŠâ€ŒØ§Ù„Ù„Ù‡</td>\n",
       "      <td>Iran</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>one person followed me // automatically checke...</td>\n",
       "      <td>2017-01-11 05:23</td>\n",
       "      <td>['http://fllwrs.com']</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>akhonfellah</td>\n",
       "      <td>â¦ğŸ‡®ğŸ‡·â©Ø£Ø®ÙŒâ€ŒÙÙŠâ€ŒØ§Ù„Ù„Ù‡</td>\n",
       "      <td>Iran</td>\n",
       "      <td>en</td>\n",
       "      <td>fa</td>\n",
       "      <td>#IDFTerrorists\\nØ­Ù…Ø§Ø³Ù‡ ØªØ±ÙˆØ±ÛŒØ³ØªÙ‡Ø§ÛŒ Ø§Ø³Ø±Ø§Ø¦ÛŒÙ„ÛŒ http...</td>\n",
       "      <td>2018-05-26 00:48</td>\n",
       "      <td>[]</td>\n",
       "      <td>['IDFTerrorists']</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>akhonfellah</td>\n",
       "      <td>â¦ğŸ‡®ğŸ‡·â©Ø£Ø®ÙŒâ€ŒÙÙŠâ€ŒØ§Ù„Ù„Ù‡</td>\n",
       "      <td>Iran</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>Stop war on Yemen hospitals\\n#ShameOnUN\\n#Yemen</td>\n",
       "      <td>2018-06-16 20:06</td>\n",
       "      <td>[]</td>\n",
       "      <td>['ShameOnUN', 'Yemen']</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>akhonfellah</td>\n",
       "      <td>â¦ğŸ‡®ğŸ‡·â©Ø£Ø®ÙŒâ€ŒÙÙŠâ€ŒØ§Ù„Ù„Ù‡</td>\n",
       "      <td>Iran</td>\n",
       "      <td>en</td>\n",
       "      <td>fa</td>\n",
       "      <td>Ù„Ø¨ÛŒÚ© ÛŒØ§ ÙÙ‚ÛŒÙ‡\\n#Ù…Ø¬Ø²Ø±Ø©_Ø§Ù„Ø¯Ø±Ø§Ø² https://t.co/nKfQW...</td>\n",
       "      <td>2018-05-23 18:22</td>\n",
       "      <td>[]</td>\n",
       "      <td>['Ù…Ø¬Ø²Ø±Ø©_Ø§Ù„Ø¯Ø±Ø§Ø²']</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>akhonfellah</td>\n",
       "      <td>â¦ğŸ‡®ğŸ‡·â©Ø£Ø®ÙŒâ€ŒÙÙŠâ€ŒØ§Ù„Ù„Ù‡</td>\n",
       "      <td>Iran</td>\n",
       "      <td>en</td>\n",
       "      <td>fa</td>\n",
       "      <td>Ø§ÛŒÙ†Ø¬Ø§ ØªÙ„ Ø§Ø¨ÛŒØ¨ Ø§Ø³Øª\\nØ§ÛŒÙ†Ù‡Ø§ Ø§Ø³Ø±Ø§ÛŒÛŒÙ„ÛŒÙ‡Ø§ÛŒÛŒ Ù‡Ø³ØªÙ†Ø¯ Ú©Ù‡...</td>\n",
       "      <td>2019-01-28 16:56</td>\n",
       "      <td>[]</td>\n",
       "      <td>['Ø²Ù†Ø¯Ú¯ÛŒ_Ø³Ú¯ÛŒ_Ø§Ø³Ø±Ø§Ø¦ÛŒÙ„ÛŒÙ‡Ø§']</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>akhonfellah</td>\n",
       "      <td>â¦ğŸ‡®ğŸ‡·â©Ø£Ø®ÙŒâ€ŒÙÙŠâ€ŒØ§Ù„Ù„Ù‡</td>\n",
       "      <td>Iran</td>\n",
       "      <td>en</td>\n",
       "      <td>ar</td>\n",
       "      <td>ÙˆØ§Ù…Ø±ÙˆØ² Ù‡Ù… ....\\n#Ø²Ù†Ø¯Ú¯ÛŒ_Ø³Ú¯ÛŒ_Ø§Ø³Ø±Ø§Ø¦ÛŒÙ„ÛŒÙ‡Ø§ https://...</td>\n",
       "      <td>2018-09-07 10:42</td>\n",
       "      <td>[]</td>\n",
       "      <td>['Ø²Ù†Ø¯Ú¯ÛŒ_Ø³Ú¯ÛŒ_Ø§Ø³Ø±Ø§Ø¦ÛŒÙ„ÛŒÙ‡Ø§']</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>akhonfellah</td>\n",
       "      <td>â¦ğŸ‡®ğŸ‡·â©Ø£Ø®ÙŒâ€ŒÙÙŠâ€ŒØ§Ù„Ù„Ù‡</td>\n",
       "      <td>Iran</td>\n",
       "      <td>en</td>\n",
       "      <td>fa</td>\n",
       "      <td>Ø­Ú©Ù…Øª Ø«Ø§Ø¨Øª Ù…ÙˆÙ†Ø¯Ù† Ø§Ø³Ù… Ù…Ø§Ù‡Ù‡Ø§ÛŒ Ù‚Ù…Ø±ÛŒ Ø¨Ø¹Ø¯ Ø§Ø² ØªØºÛŒÛŒØ± Ø²...</td>\n",
       "      <td>2017-11-19 19:40</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>akhonfellah</td>\n",
       "      <td>â¦ğŸ‡®ğŸ‡·â©Ø£Ø®ÙŒâ€ŒÙÙŠâ€ŒØ§Ù„Ù„Ù‡</td>\n",
       "      <td>Iran</td>\n",
       "      <td>en</td>\n",
       "      <td>fa</td>\n",
       "      <td>Ø¬Ù…Ù„Ù‡ Ø§ÛŒ Ú©Ù‡ Ø³ÛŒØ¯Ø­Ø³Ù† Ø§Ù…Ø´Ø¨ Ú¯ÙØª Ø¯Ø± Ù…ÙˆØ±Ø¯ Ø§Ù†Ù‚Ù„Ø§Ø¨ Ø§Ø³Ù„Ø§...</td>\n",
       "      <td>2019-02-06 18:26</td>\n",
       "      <td>[]</td>\n",
       "      <td>['Ø¥Ù†_Ù…Ø¹_Ø§Ù„ØµØ¨Ø±_Ù†ØµØ±Ø§']</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>akhonfellah</td>\n",
       "      <td>â¦ğŸ‡®ğŸ‡·â©Ø£Ø®ÙŒâ€ŒÙÙŠâ€ŒØ§Ù„Ù„Ù‡</td>\n",
       "      <td>Iran</td>\n",
       "      <td>en</td>\n",
       "      <td>fa</td>\n",
       "      <td>Ø¨Ø´Ù†ÙˆÛŒØ¯ Ù…Ø¯Ø­ Ø­Ø§Ø¬ Ù…Ø­Ù…ÙˆØ¯Ø¢Ù‚ÙˆÛŒ Ú©Ø±ÛŒÙ…ÛŒ Ø±Ùˆ Ø¨Ø§ Ù„Ù‡Ø¬Ù‡ Ø´ÛŒØ±Ø§...</td>\n",
       "      <td>2017-08-04 12:25</td>\n",
       "      <td>['https://twitter.com/khanisadiq/status/893438...</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>akhonfellah</td>\n",
       "      <td>â¦ğŸ‡®ğŸ‡·â©Ø£Ø®ÙŒâ€ŒÙÙŠâ€ŒØ§Ù„Ù„Ù‡</td>\n",
       "      <td>Iran</td>\n",
       "      <td>en</td>\n",
       "      <td>ar</td>\n",
       "      <td>RT @awadazeinab1: ÙƒÙ… Ø³Ø§Ø¹Ø© Ù…Ø¹ Ø§Ø¨Ù†ÙŠ Ø¨Ø§Ù„Ù…Ø³ØªØ´ÙÙ‰ Ø´Ù...</td>\n",
       "      <td>2019-01-25 17:48</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_screen_name user_display_name user_reported_location account_language  \\\n",
       "0      akhonfellah   â¦ğŸ‡®ğŸ‡·â©Ø£Ø®ÙŒâ€ŒÙÙŠâ€ŒØ§Ù„Ù„Ù‡                   Iran               en   \n",
       "1      akhonfellah   â¦ğŸ‡®ğŸ‡·â©Ø£Ø®ÙŒâ€ŒÙÙŠâ€ŒØ§Ù„Ù„Ù‡                   Iran               en   \n",
       "2      akhonfellah   â¦ğŸ‡®ğŸ‡·â©Ø£Ø®ÙŒâ€ŒÙÙŠâ€ŒØ§Ù„Ù„Ù‡                   Iran               en   \n",
       "3      akhonfellah   â¦ğŸ‡®ğŸ‡·â©Ø£Ø®ÙŒâ€ŒÙÙŠâ€ŒØ§Ù„Ù„Ù‡                   Iran               en   \n",
       "4      akhonfellah   â¦ğŸ‡®ğŸ‡·â©Ø£Ø®ÙŒâ€ŒÙÙŠâ€ŒØ§Ù„Ù„Ù‡                   Iran               en   \n",
       "5      akhonfellah   â¦ğŸ‡®ğŸ‡·â©Ø£Ø®ÙŒâ€ŒÙÙŠâ€ŒØ§Ù„Ù„Ù‡                   Iran               en   \n",
       "6      akhonfellah   â¦ğŸ‡®ğŸ‡·â©Ø£Ø®ÙŒâ€ŒÙÙŠâ€ŒØ§Ù„Ù„Ù‡                   Iran               en   \n",
       "7      akhonfellah   â¦ğŸ‡®ğŸ‡·â©Ø£Ø®ÙŒâ€ŒÙÙŠâ€ŒØ§Ù„Ù„Ù‡                   Iran               en   \n",
       "8      akhonfellah   â¦ğŸ‡®ğŸ‡·â©Ø£Ø®ÙŒâ€ŒÙÙŠâ€ŒØ§Ù„Ù„Ù‡                   Iran               en   \n",
       "9      akhonfellah   â¦ğŸ‡®ğŸ‡·â©Ø£Ø®ÙŒâ€ŒÙÙŠâ€ŒØ§Ù„Ù„Ù‡                   Iran               en   \n",
       "\n",
       "  tweet_language                                         tweet_text  \\\n",
       "0             en  one person followed me // automatically checke...   \n",
       "1             fa  #IDFTerrorists\\nØ­Ù…Ø§Ø³Ù‡ ØªØ±ÙˆØ±ÛŒØ³ØªÙ‡Ø§ÛŒ Ø§Ø³Ø±Ø§Ø¦ÛŒÙ„ÛŒ http...   \n",
       "2             en    Stop war on Yemen hospitals\\n#ShameOnUN\\n#Yemen   \n",
       "3             fa  Ù„Ø¨ÛŒÚ© ÛŒØ§ ÙÙ‚ÛŒÙ‡\\n#Ù…Ø¬Ø²Ø±Ø©_Ø§Ù„Ø¯Ø±Ø§Ø² https://t.co/nKfQW...   \n",
       "4             fa  Ø§ÛŒÙ†Ø¬Ø§ ØªÙ„ Ø§Ø¨ÛŒØ¨ Ø§Ø³Øª\\nØ§ÛŒÙ†Ù‡Ø§ Ø§Ø³Ø±Ø§ÛŒÛŒÙ„ÛŒÙ‡Ø§ÛŒÛŒ Ù‡Ø³ØªÙ†Ø¯ Ú©Ù‡...   \n",
       "5             ar  ÙˆØ§Ù…Ø±ÙˆØ² Ù‡Ù… ....\\n#Ø²Ù†Ø¯Ú¯ÛŒ_Ø³Ú¯ÛŒ_Ø§Ø³Ø±Ø§Ø¦ÛŒÙ„ÛŒÙ‡Ø§ https://...   \n",
       "6             fa  Ø­Ú©Ù…Øª Ø«Ø§Ø¨Øª Ù…ÙˆÙ†Ø¯Ù† Ø§Ø³Ù… Ù…Ø§Ù‡Ù‡Ø§ÛŒ Ù‚Ù…Ø±ÛŒ Ø¨Ø¹Ø¯ Ø§Ø² ØªØºÛŒÛŒØ± Ø²...   \n",
       "7             fa  Ø¬Ù…Ù„Ù‡ Ø§ÛŒ Ú©Ù‡ Ø³ÛŒØ¯Ø­Ø³Ù† Ø§Ù…Ø´Ø¨ Ú¯ÙØª Ø¯Ø± Ù…ÙˆØ±Ø¯ Ø§Ù†Ù‚Ù„Ø§Ø¨ Ø§Ø³Ù„Ø§...   \n",
       "8             fa  Ø¨Ø´Ù†ÙˆÛŒØ¯ Ù…Ø¯Ø­ Ø­Ø§Ø¬ Ù…Ø­Ù…ÙˆØ¯Ø¢Ù‚ÙˆÛŒ Ú©Ø±ÛŒÙ…ÛŒ Ø±Ùˆ Ø¨Ø§ Ù„Ù‡Ø¬Ù‡ Ø´ÛŒØ±Ø§...   \n",
       "9             ar  RT @awadazeinab1: ÙƒÙ… Ø³Ø§Ø¹Ø© Ù…Ø¹ Ø§Ø¨Ù†ÙŠ Ø¨Ø§Ù„Ù…Ø³ØªØ´ÙÙ‰ Ø´Ù...   \n",
       "\n",
       "         tweet_time                                               urls  \\\n",
       "0  2017-01-11 05:23                              ['http://fllwrs.com']   \n",
       "1  2018-05-26 00:48                                                 []   \n",
       "2  2018-06-16 20:06                                                 []   \n",
       "3  2018-05-23 18:22                                                 []   \n",
       "4  2019-01-28 16:56                                                 []   \n",
       "5  2018-09-07 10:42                                                 []   \n",
       "6  2017-11-19 19:40                                                 []   \n",
       "7  2019-02-06 18:26                                                 []   \n",
       "8  2017-08-04 12:25  ['https://twitter.com/khanisadiq/status/893438...   \n",
       "9  2019-01-25 17:48                                                 []   \n",
       "\n",
       "                   hashtags  is_retweet  \n",
       "0                        []       False  \n",
       "1         ['IDFTerrorists']       False  \n",
       "2    ['ShameOnUN', 'Yemen']       False  \n",
       "3          ['Ù…Ø¬Ø²Ø±Ø©_Ø§Ù„Ø¯Ø±Ø§Ø²']       False  \n",
       "4  ['Ø²Ù†Ø¯Ú¯ÛŒ_Ø³Ú¯ÛŒ_Ø§Ø³Ø±Ø§Ø¦ÛŒÙ„ÛŒÙ‡Ø§']       False  \n",
       "5  ['Ø²Ù†Ø¯Ú¯ÛŒ_Ø³Ú¯ÛŒ_Ø§Ø³Ø±Ø§Ø¦ÛŒÙ„ÛŒÙ‡Ø§']       False  \n",
       "6                        []       False  \n",
       "7      ['Ø¥Ù†_Ù…Ø¹_Ø§Ù„ØµØ¨Ø±_Ù†ØµØ±Ø§']       False  \n",
       "8                        []       False  \n",
       "9                        []        True  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_clean = tweets[['user_screen_name',  'user_display_name', 'user_reported_location', 'account_language', 'tweet_language', 'tweet_text', 'tweet_time', 'urls', 'hashtags', 'is_retweet']]\n",
    "\n",
    "tweets_clean.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c). Filter tweets and keep those that are\n",
    "- account location in Venezuela\n",
    "- account language is Spanish or\n",
    "- Tweet language is Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_clean = tweets_clean[(tweets_clean.user_reported_location == 'Venezuela') | (tweets_clean.account_language == 'es') | (tweets_clean.tweet_language == 'es')].reset_index().drop(columns= ['index'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d.) Take out tweets that are set in European and US locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_clean = tweets_clean[(tweets_clean.user_reported_location != 'London') & (tweets_clean.user_reported_location != 'Manhattan, NY') & (tweets_clean.user_reported_location != 'Brooklyn, NY') & (tweets_clean.user_reported_location != 'Queens, NY') & (tweets_clean.user_reported_location != 'New York, NY') & (tweets_clean.user_reported_location != 'California, USA') & (tweets_clean.user_reported_location != 'New Jersey, USA') &  (tweets_clean.user_reported_location != 'North Holland, The Netherlands') & (tweets_clean.user_reported_location != 'Atlantic City, NJ') & (tweets_clean.user_reported_location != 'Mountain View, CA') & (tweets_clean.user_reported_location != 'New York, USA') & (tweets_clean.user_reported_location != 'Canada') & (tweets_clean.user_reported_location != 'San Francisco, CA') & (tweets_clean.user_reported_location != 'Washington, USA') & (tweets_clean.user_reported_location != 'Washington, DC') & (tweets_clean.user_reported_location != 'EspaÃ±a') & (tweets_clean.user_reported_location != 'Germany') & (tweets_clean.user_reported_location != 'Nantes, France') & (tweets_clean.user_reported_location != 'Houston, TX') & (tweets_clean.user_reported_location != 'Texas,San Antonio') & (tweets_clean.user_reported_location != 'Chicago') & (tweets_clean.user_reported_location != 'Atlanta') & (tweets_clean.user_reported_location != 'Washington,Seattle') & (tweets_clean.user_reported_location != 'Fremont, CA') & (tweets_clean.user_reported_location != 'France') & (tweets_clean.user_reported_location != 'England, United Kingdom')  & (tweets_clean.user_reported_location != 'Oregon,Portland')  & (tweets_clean.user_reported_location !='USA')  & (tweets_clean.user_reported_location != 'Florida,Orlando') & (tweets_clean.user_reported_location != 'Califor') & (tweets_clean.user_reported_location !='California,Los Angeles') & (tweets_clean.user_reported_location !='Illinois, USA') & (tweets_clean.user_reported_location !='Arizona,phoenix') & (tweets_clean.user_reported_location !='Pennsylvania,Pittsburgh') & (tweets_clean.user_reported_location !='Pennsylvania,Philadelphia') & (tweets_clean.user_reported_location !='Dallas, TX') ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e.) Quality control for Tweet Language\n",
    "\n",
    "- Use SpaCy language identification to cross-check Twitterâ€™s language identification. Identify language of each tweet using SpaCy.\n",
    "- Exclude tweets on which SpaCy and Twitter don't agree on the language\n",
    "- Spanish and English datasets consist of Tweets that were marked as either Spanish by BOTH Twitter and SpaCy or English by BOTH Twitter and SpaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy_langdetect import LanguageDetector\n",
    "\n",
    "\n",
    "# Initialize spacy with the SPANISH model\n",
    "sp = spacy.load('es_core_news_sm')\n",
    "sp.add_pipe(LanguageDetector(), name = 'language_detector', last = True)\n",
    "eng = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_language(df, content_column):\n",
    "    '''\n",
    "    A function to detect the language in each tweet and add to new row\n",
    "\n",
    "    Argument: a dataframe  and content column\n",
    "    Ouput: same dataframe with a new 'cleaned_content' column\n",
    "    '''\n",
    "\n",
    "    # Initialize list of languages\n",
    "    spacy_language_detection = []\n",
    "\n",
    "    # Call detect the language for each row in the data frame and append to spacy_language_detection list\n",
    "    for row in df[content_column]:\n",
    "        doc = sp(row)\n",
    "        spacy_language_detection.append(doc._.language['language'])\n",
    "\n",
    "    # Append language list to the data frame\n",
    "    df['spacy_language_detection'] = spacy_language_detection\n",
    "\n",
    "    return df \n",
    "\n",
    "tweets_clean = detect_language(df = tweets_clean, content_column = 'tweet_text')\n",
    "\n",
    "# Isolate tweets marked as Spanish by Twitter AND SpaCy \n",
    "spanish_tweets = tweets_clean[(tweets_clean.tweet_language == 'es') & (tweets_clean.spacy_language_detection == 'es')]\n",
    "\n",
    "\n",
    "# Isolate tweets marked as English by Twitter AND SpaCy \n",
    "english_tweets = tweets_clean[(tweets_clean.tweet_language == 'en') & (tweets_clean.spacy_language_detection == 'en')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_tweets = pd.read_csv('proj_data/spanish_tweets.csv', encoding='utf-8-sig', index_col = 0).reset_index()\n",
    "english_tweets = pd.read_csv('proj_data/english_tweets.csv', encoding='utf-8-sig', index_col = 0).reset_index()\n",
    "english_tweets.lemmatized_tweet_text = english_tweets.lemmatized_tweet_text.astype('str')\n",
    "spanish_tweets.lemmatized_tweet_text = spanish_tweets.lemmatized_tweet_text.astype('str')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Keyword analysis\n",
    "\n",
    "### a.) Lemmatize and simplify Tweet text\n",
    "- â€œCleanâ€ each Tweet by removing stop words (very common words that carry little meaning), non alpha-numeric characters, and by reducing each word to itâ€™s lemma or root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(text_string, language):\n",
    "    '''\n",
    "    A function to clean a string using SpaCy, removing stop-words and non-alphanumeric characters\n",
    "\n",
    "    Argument: a text string and a language ('English' or 'Spanish')\n",
    "    Output: a cleaned string\n",
    "\n",
    "    '''\n",
    "    if language == 'Spanish':\n",
    "    # Parse the text string using the english model initialized earlier\n",
    "        doc = sp(text_string)\n",
    "    elif language == 'English':\n",
    "        doc = eng(text_string)\n",
    "    \n",
    "    # Initialize empty string\n",
    "    clean = []\n",
    "\n",
    "    # Add each token to the list if it is not a stop word, is alphanumeric, and if it's not a pronoun\n",
    "    for token in doc:\n",
    "        \n",
    "        if token.is_alpha == False or token.is_stop == True:\n",
    "            pass\n",
    "        else:\n",
    "            clean.append(token.lemma_)\n",
    "\n",
    "    # Join the list into a string\n",
    "    clean = \" \".join(clean)\n",
    "\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example of how the cleaning works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw example: \n",
      "RT @Algbarow55: Solo en #Yemen los padres pierden a sus hijos; NiâˆšÂ±os pierden a sus padres #SaudiBombsChildren https://t.co/uImWl8QG5u\n",
      "\n",
      "\n",
      "Clean exmaple: \n",
      "RT Solo Yemen padre perder a hijo perder a padre SaudiBombsChildren\n"
     ]
    }
   ],
   "source": [
    "example_sp = spanish_tweets.loc[2,'tweet_text']\n",
    "example_sp_clean = clean_string(example_sp, \"Spanish\")\n",
    "print(\"Raw example: \\n\" + example_sp)\n",
    "print(\"\\n\\nClean exmaple: \\n\" + example_sp_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_content(df, content_column, language):\n",
    "    '''\n",
    "    A function to clean all the strings in a whole of a corpus\n",
    "\n",
    "    Argument: a dataframe, the name of the column with the content, and a language ('Spanish' or 'English')\n",
    "    Ouput: same dataframe with a new cleaned content column\n",
    "    '''\n",
    "\n",
    "    # Initialize list of cleaned content strings\n",
    "    clean_content= []\n",
    "\n",
    "    # Call clean_string() for each row in the data frame and append to clean_content list\n",
    "    for row in df[content_column]:\n",
    "        clean_content.append(clean_string(row, language))\n",
    "\n",
    "    # Append clean_content list to the data frame\n",
    "    df['lemmatized_tweet_text'] = clean_content\n",
    "\n",
    "    return df \n",
    "\n",
    "spanish_tweets = clean_content(spanish_tweets, 'tweet_text', 'Spanish')\n",
    "\n",
    "english_tweets = clean_content(english_tweets, 'tweet_text', 'English')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b.) Create word counts\n",
    "\n",
    "Use Scikit-Learn CountVectorizer create a matrix where each column is a word found in any Tweet and each row is the number of times it occurs in each Tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only words that appear in more than 5 tweets (a way to decrease the size of the vocab)\n",
    "word_vectorizer = CountVectorizer(encoding='utf-8-sig', analyzer='word', min_df=5, ngram_range=(1,1))\n",
    "# create matrix where each column is a word and each row is a count in each tweet\n",
    "word_count_sm = word_vectorizer.fit_transform(spanish_tweets['lemmatized_tweet_text'])\n",
    "words = word_vectorizer.get_feature_names()\n",
    "\n",
    "# sum the count of each word over all Tweets\n",
    "word_count_total = word_count_sm.sum(axis=0)\n",
    "word_count_total_df = pd.DataFrame(word_count_total, columns = words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abadi</th>\n",
       "      <th>abajar</th>\n",
       "      <th>abajo</th>\n",
       "      <th>abandonar</th>\n",
       "      <th>abatir</th>\n",
       "      <th>abbas</th>\n",
       "      <th>abby</th>\n",
       "      <th>abc</th>\n",
       "      <th>abdel</th>\n",
       "      <th>abdullah</th>\n",
       "      <th>...</th>\n",
       "      <th>Ã¤Ãºpor</th>\n",
       "      <th>Ã¤Ãºsegundo</th>\n",
       "      <th>Ã¤Ãºsionistas</th>\n",
       "      <th>Ã¤Ãºtsunami</th>\n",
       "      <th>Ã¤Ãºun</th>\n",
       "      <th>Ã¤Ãºyo</th>\n",
       "      <th>Ã²Ã«</th>\n",
       "      <th>Ã²Ã¯</th>\n",
       "      <th>Ã¹Ã¥</th>\n",
       "      <th>Ã¼Ã­</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>8</td>\n",
       "      <td>79</td>\n",
       "      <td>50</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>26</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 6750 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abadi  abajar  abajo  abandonar  abatir  abbas  abby  abc  abdel  abdullah  \\\n",
       "0     18      19      8         79      50     15     6    5     17        10   \n",
       "\n",
       "   ...  Ã¤Ãºpor  Ã¤Ãºsegundo  Ã¤Ãºsionistas  Ã¤Ãºtsunami  Ã¤Ãºun  Ã¤Ãºyo  Ã²Ã«  Ã²Ã¯  Ã¹Ã¥  Ã¼Ã­  \n",
       "0  ...      5          6            6         18    15    11   6  13  26  14  \n",
       "\n",
       "[1 rows x 6750 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count_total_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c.) Phrase counts\n",
    "\n",
    "Change the number of words analyzed with the same type of vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only phrases that appear in more than 5 tweets (a way to decrease the size of the vocab)\n",
    "phrase_vectorizer = CountVectorizer(encoding='utf-8-sig', analyzer='word', min_df=5, ngram_range=(5,7))\n",
    "# create matrix where each column is a phrase and each row is a count in each tweet\n",
    "phrase_count_sm = phrase_vectorizer.fit_transform(spanish_tweets['lemmatized_tweet_text'])\n",
    "phrases = phrase_vectorizer.get_feature_names()\n",
    "# sum the count of each phrase over all Tweets\n",
    "phrase_count_total = phrase_count_sm.sum(axis=0)\n",
    "phrase_count_total_df = pd.DataFrame(phrase_count_total, columns = phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abby martin exponer fascista colonial</th>\n",
       "      <th>abby martin exponer fascista colonial israel</th>\n",
       "      <th>abby martin exponer fascista colonial israel sionismo</th>\n",
       "      <th>abdel rahman hussein abu hmash</th>\n",
       "      <th>abdel rahman hussein abu hmash palestino</th>\n",
       "      <th>abdel rahman hussein abu hmash palestino asesinar</th>\n",
       "      <th>abogado hermann el pdte llamar</th>\n",
       "      <th>abogado hermann el pdte llamar reflexionar</th>\n",
       "      <th>abogado hermann el pdte llamar reflexionar pensar</th>\n",
       "      <th>abogados casar narvarte objetivo malware</th>\n",
       "      <th>...</th>\n",
       "      <th>Ã¤Ãºes fuerza seguridad estado entrar</th>\n",
       "      <th>Ã¤Ãºes fuerza seguridad estado entrar perseguir</th>\n",
       "      <th>Ã¤Ãºgobernadores adversar gobierno renunciar resignarse</th>\n",
       "      <th>Ã¤Ãºgobernadores adversar gobierno renunciar resignarse titular</th>\n",
       "      <th>Ã¤Ãºno tarea segundo importante general</th>\n",
       "      <th>Ã¤Ãºno tarea segundo importante general promover</th>\n",
       "      <th>Ã¤Ãºno tarea segundo importante general promover proteger</th>\n",
       "      <th>Ã¤Ãºyo desear segundo formar rico</th>\n",
       "      <th>Ã¤Ãºyo desear segundo formar rico segundo</th>\n",
       "      <th>Ã¤Ãºyo desear segundo formar rico segundo grande</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 8746 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abby martin exponer fascista colonial  \\\n",
       "0                                      6   \n",
       "\n",
       "   abby martin exponer fascista colonial israel  \\\n",
       "0                                             6   \n",
       "\n",
       "   abby martin exponer fascista colonial israel sionismo  \\\n",
       "0                                                  6       \n",
       "\n",
       "   abdel rahman hussein abu hmash  abdel rahman hussein abu hmash palestino  \\\n",
       "0                              10                                        10   \n",
       "\n",
       "   abdel rahman hussein abu hmash palestino asesinar  \\\n",
       "0                                                 10   \n",
       "\n",
       "   abogado hermann el pdte llamar  abogado hermann el pdte llamar reflexionar  \\\n",
       "0                              14                                          14   \n",
       "\n",
       "   abogado hermann el pdte llamar reflexionar pensar  \\\n",
       "0                                                 14   \n",
       "\n",
       "   abogados casar narvarte objetivo malware  ...  \\\n",
       "0                                         5  ...   \n",
       "\n",
       "   Ã¤Ãºes fuerza seguridad estado entrar  \\\n",
       "0                                    6   \n",
       "\n",
       "   Ã¤Ãºes fuerza seguridad estado entrar perseguir  \\\n",
       "0                                              6   \n",
       "\n",
       "   Ã¤Ãºgobernadores adversar gobierno renunciar resignarse  \\\n",
       "0                                                  7       \n",
       "\n",
       "   Ã¤Ãºgobernadores adversar gobierno renunciar resignarse titular  \\\n",
       "0                                                  7               \n",
       "\n",
       "   Ã¤Ãºno tarea segundo importante general  \\\n",
       "0                                      8   \n",
       "\n",
       "   Ã¤Ãºno tarea segundo importante general promover  \\\n",
       "0                                               8   \n",
       "\n",
       "   Ã¤Ãºno tarea segundo importante general promover proteger  \\\n",
       "0                                                  8         \n",
       "\n",
       "   Ã¤Ãºyo desear segundo formar rico  Ã¤Ãºyo desear segundo formar rico segundo  \\\n",
       "0                                6                                        6   \n",
       "\n",
       "   Ã¤Ãºyo desear segundo formar rico segundo grande  \n",
       "0                                               6  \n",
       "\n",
       "[1 rows x 8746 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase_count_total_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d.) Keyword analysis among Tweets that mention Venezuela or Venezolano/a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate Tweets that mention Venezuela or Venezolano/a\n",
    "spanish_tweets_venezuela = spanish_tweets[spanish_tweets.tweet_text.str.contains(\"venez\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Hashtag Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove parens and filter hashtags\n",
    "hashtags = []\n",
    "for hashtag in spanish_tweets.hashtags:\n",
    "    if type(hashtag) == float:\n",
    "        hashtags.append('None')\n",
    "    elif len(hashtag) < 3:\n",
    "        hashtags.append('None')\n",
    "    else:\n",
    "        hashtags.append(hashtag[1:-1])\n",
    "        \n",
    "# Add filtered hashtags back to data frame\n",
    "spanish_tweets.hashtags = hashtags\n",
    "\n",
    "# Spanish hashtag count and save \n",
    "hashtag_vectorizer = CountVectorizer(encoding='utf-8-sig', analyzer='word', ngram_range=(1,1))\n",
    "hashtag_count_sm = hashtag_vectorizer.fit_transform(spanish_tweets['hashtags'])\n",
    "hashtags = hashtag_vectorizer.get_feature_names()\n",
    "hashtag_total = hashtag_count_sm.sum(axis = 0)\n",
    "hashtag_count_df = pd.DataFrame(hashtag_total, columns= hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>01jul</th>\n",
       "      <th>02ago</th>\n",
       "      <th>02feb</th>\n",
       "      <th>03sep</th>\n",
       "      <th>04ago</th>\n",
       "      <th>04nov</th>\n",
       "      <th>06agos</th>\n",
       "      <th>06agosto</th>\n",
       "      <th>09may</th>\n",
       "      <th>09sep</th>\n",
       "      <th>...</th>\n",
       "      <th>Ã¿Ã¤Ã¿Ã¼</th>\n",
       "      <th>Ã¿Ã¤Ã¿Ã¼_Ã¿ÃŸÃ¿Ã±Ã¿</th>\n",
       "      <th>Ã¿Ã¦Ã¿</th>\n",
       "      <th>Ã¿Ã©_Ã¿Ï€Ã¿Ã¼Ã¿Ã¸_Ã¿ÃŸÃ¿ÂµÃ¿Ã¸Ã¿Ã§Ã¿ÃŸÃ¿Ã¤Ã¿Ã©</th>\n",
       "      <th>Ã¿Ã±Ã¿Ã§Ã¿</th>\n",
       "      <th>Ã¿Ã¸</th>\n",
       "      <th>Ã¿Ã¸Ã¿Ã¤Ã¿</th>\n",
       "      <th>Ã¿Ã¼</th>\n",
       "      <th>Ã¿Ã¼_Ã¿</th>\n",
       "      <th>Ã¿Ã¼Ã¿Ã Ã¿ÃŸÃ¿</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 5476 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   01jul  02ago  02feb  03sep  04ago  04nov  06agos  06agosto  09may  09sep  \\\n",
       "0      2      4      9      8      2      2       2         2      1      5   \n",
       "\n",
       "   ...  Ã¿Ã¤Ã¿Ã¼  Ã¿Ã¤Ã¿Ã¼_Ã¿ÃŸÃ¿Ã±Ã¿  Ã¿Ã¦Ã¿  Ã¿Ã©_Ã¿Ï€Ã¿Ã¼Ã¿Ã¸_Ã¿ÃŸÃ¿ÂµÃ¿Ã¸Ã¿Ã§Ã¿ÃŸÃ¿Ã¤Ã¿Ã©  Ã¿Ã±Ã¿Ã§Ã¿  Ã¿Ã¸  Ã¿Ã¸Ã¿Ã¤Ã¿  Ã¿Ã¼  \\\n",
       "0  ...     2           2    4                         2      2   6      2   1   \n",
       "\n",
       "   Ã¿Ã¼_Ã¿  Ã¿Ã¼Ã¿Ã Ã¿ÃŸÃ¿  \n",
       "0     3        1  \n",
       "\n",
       "[1 rows x 5476 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashtag_count_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Link Farming Analysis\n",
    "\n",
    "What proportion of Tweets follow the typical link farming pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_tweets = pd.read_csv('proj_data/spanish_tweets.csv', encoding='utf-8-sig', index_col = 0).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 27677 Tweets with links\n",
      "There are 19215 Tweets without links\n"
     ]
    }
   ],
   "source": [
    "# Replace tweets without link with Null value\n",
    "spanish_tweets.urls = spanish_tweets.urls.replace({'[]': np.nan})\n",
    "\n",
    "\n",
    "tweets_with_link = 0\n",
    "tweets_wout_link = 0\n",
    "\n",
    "for url in spanish_tweets.urls:\n",
    "    if pd.isnull(url) == True:\n",
    "        tweets_with_link +=1\n",
    "    else:\n",
    "        tweets_wout_link += 1\n",
    "        \n",
    "print(\"There are \" + str(tweets_with_link) + \" Tweets with links\")\n",
    "print(\"There are \" + str(tweets_wout_link) + \" Tweets without links\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13858 of the Tweets with links mention other accounts\n"
     ]
    }
   ],
   "source": [
    "spanish_tweets_url  = spanish_tweets[pd.notnull(spanish_tweets.urls)]\n",
    "spanish_tweets_url_at = spanish_tweets_url[spanish_tweets_url.tweet_text.str.contains('@')]\n",
    "print(str(len(spanish_tweets_url_at)) + \" of the Tweets with links mention other accounts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9716 of the Tweets with links are retweets that mention other accounts\n"
     ]
    }
   ],
   "source": [
    "spanish_tweets_url_at_rt = spanish_tweets_url_at[spanish_tweets_url_at.tweet_text.str.startswith('RT')]\n",
    "print(str(len(spanish_tweets_url_at_rt)) + \" of the Tweets with links are retweets that mention other accounts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_tweets_no_content_copy = spanish_tweets.tweet_text.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 46892 total tweets\n"
     ]
    }
   ],
   "source": [
    "print(\"There are \" + str(len(spanish_tweets)) + \" total tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 32014 unique tweets; therefore, 14878 are duplicates.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are \" + str(len(spanish_tweets_no_content_copy)) + \" unique tweets; therefore, \" + str(len(spanish_tweets)- len(spanish_tweets_no_content_copy)) + \" are duplicates.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Date analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_date = []\n",
    "for date in spanish_tweets.tweet_time:\n",
    "    tweet_date.append(str(date)[:-5].strip())\n",
    "\n",
    "spanish_tweets['tweet_date'] = tweet_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_per_day = spanish_tweets.groupby('tweet_date')[['tweet_text']].count()\n",
    "tweets_per_day.to_csv('proj_data/tweets_per_day.csv', encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweet_date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1/1/16</th>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1/1/17</th>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1/1/18</th>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1/10/16</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9/7/17</th>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9/8/16</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9/8/17</th>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9/9/16</th>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9/9/17</th>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1184 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            tweet_text\n",
       "tweet_date            \n",
       "                     2\n",
       "1/1/16              53\n",
       "1/1/17              39\n",
       "1/1/18              68\n",
       "1/10/16              6\n",
       "...                ...\n",
       "9/7/17              99\n",
       "9/8/16               8\n",
       "9/8/17              65\n",
       "9/9/16              18\n",
       "9/9/17              85\n",
       "\n",
       "[1184 rows x 1 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_per_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
