{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disinformation on twitter\n",
    "Iranian state-sponsored campaigns aimed at Venezuela\n",
    "\n",
    "\n",
    "## Step 1: Data Cleaning\n",
    "#### This chunk of code just imports some programs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This lets the program know which is the current folder and which is the folder with the data in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJ_ROOT_DIR = os.getcwd()\n",
    "\n",
    "DATA_PATH = os.path.join(PROJ_ROOT_DIR, \"csv\")\n",
    "if not os.path.isdir(DATA_PATH):  \n",
    "    os.makedirs(DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a.) Combine all the data files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_twitter_data():\n",
    "    \"\"\"\n",
    "    A function to load data from data folder\n",
    "    \"\"\"\n",
    "    # List of files\n",
    "    files = [f for f in os.listdir(DATA_PATH) if f.endswith(\".csv\")]\n",
    "    \n",
    "    # List of data frames\n",
    "    file_list = []\n",
    "    \n",
    "    # Append each data frame in files to the file_list\n",
    "    for filename in files:\n",
    "        df = pd.read_csv(os.path.join(DATA_PATH, filename), low_memory=False)\n",
    "        file_list.append(df)\n",
    "        \n",
    "    # Concatenate all the news data frames\n",
    "    df_full = pd.concat(file_list, join='outer', sort = True).drop_duplicates().reset_index().drop(columns='index')\n",
    "    \n",
    "    return df_full\n",
    "\n",
    "tweets = load_twitter_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b.) Select only the columns we are interested in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_screen_name</th>\n",
       "      <th>user_display_name</th>\n",
       "      <th>user_reported_location</th>\n",
       "      <th>account_language</th>\n",
       "      <th>tweet_language</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>tweet_time</th>\n",
       "      <th>urls</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>is_retweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>akhonfellah</td>\n",
       "      <td>â¦ğŸ‡®ğŸ‡·â©Ø£Ø®ÙŒâ€ŒÙÙŠâ€ŒØ§Ù„Ù„Ù‡</td>\n",
       "      <td>Iran</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>one person followed me // automatically checke...</td>\n",
       "      <td>2017-01-11 05:23</td>\n",
       "      <td>['http://fllwrs.com']</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>akhonfellah</td>\n",
       "      <td>â¦ğŸ‡®ğŸ‡·â©Ø£Ø®ÙŒâ€ŒÙÙŠâ€ŒØ§Ù„Ù„Ù‡</td>\n",
       "      <td>Iran</td>\n",
       "      <td>en</td>\n",
       "      <td>fa</td>\n",
       "      <td>#IDFTerrorists\\nØ­Ù…Ø§Ø³Ù‡ ØªØ±ÙˆØ±ÛŒØ³ØªÙ‡Ø§ÛŒ Ø§Ø³Ø±Ø§Ø¦ÛŒÙ„ÛŒ http...</td>\n",
       "      <td>2018-05-26 00:48</td>\n",
       "      <td>[]</td>\n",
       "      <td>['IDFTerrorists']</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>akhonfellah</td>\n",
       "      <td>â¦ğŸ‡®ğŸ‡·â©Ø£Ø®ÙŒâ€ŒÙÙŠâ€ŒØ§Ù„Ù„Ù‡</td>\n",
       "      <td>Iran</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>Stop war on Yemen hospitals\\n#ShameOnUN\\n#Yemen</td>\n",
       "      <td>2018-06-16 20:06</td>\n",
       "      <td>[]</td>\n",
       "      <td>['ShameOnUN', 'Yemen']</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>akhonfellah</td>\n",
       "      <td>â¦ğŸ‡®ğŸ‡·â©Ø£Ø®ÙŒâ€ŒÙÙŠâ€ŒØ§Ù„Ù„Ù‡</td>\n",
       "      <td>Iran</td>\n",
       "      <td>en</td>\n",
       "      <td>fa</td>\n",
       "      <td>Ù„Ø¨ÛŒÚ© ÛŒØ§ ÙÙ‚ÛŒÙ‡\\n#Ù…Ø¬Ø²Ø±Ø©_Ø§Ù„Ø¯Ø±Ø§Ø² https://t.co/nKfQW...</td>\n",
       "      <td>2018-05-23 18:22</td>\n",
       "      <td>[]</td>\n",
       "      <td>['Ù…Ø¬Ø²Ø±Ø©_Ø§Ù„Ø¯Ø±Ø§Ø²']</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>akhonfellah</td>\n",
       "      <td>â¦ğŸ‡®ğŸ‡·â©Ø£Ø®ÙŒâ€ŒÙÙŠâ€ŒØ§Ù„Ù„Ù‡</td>\n",
       "      <td>Iran</td>\n",
       "      <td>en</td>\n",
       "      <td>fa</td>\n",
       "      <td>Ø§ÛŒÙ†Ø¬Ø§ ØªÙ„ Ø§Ø¨ÛŒØ¨ Ø§Ø³Øª\\nØ§ÛŒÙ†Ù‡Ø§ Ø§Ø³Ø±Ø§ÛŒÛŒÙ„ÛŒÙ‡Ø§ÛŒÛŒ Ù‡Ø³ØªÙ†Ø¯ Ú©Ù‡...</td>\n",
       "      <td>2019-01-28 16:56</td>\n",
       "      <td>[]</td>\n",
       "      <td>['Ø²Ù†Ø¯Ú¯ÛŒ_Ø³Ú¯ÛŒ_Ø§Ø³Ø±Ø§Ø¦ÛŒÙ„ÛŒÙ‡Ø§']</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>akhonfellah</td>\n",
       "      <td>â¦ğŸ‡®ğŸ‡·â©Ø£Ø®ÙŒâ€ŒÙÙŠâ€ŒØ§Ù„Ù„Ù‡</td>\n",
       "      <td>Iran</td>\n",
       "      <td>en</td>\n",
       "      <td>ar</td>\n",
       "      <td>ÙˆØ§Ù…Ø±ÙˆØ² Ù‡Ù… ....\\n#Ø²Ù†Ø¯Ú¯ÛŒ_Ø³Ú¯ÛŒ_Ø§Ø³Ø±Ø§Ø¦ÛŒÙ„ÛŒÙ‡Ø§ https://...</td>\n",
       "      <td>2018-09-07 10:42</td>\n",
       "      <td>[]</td>\n",
       "      <td>['Ø²Ù†Ø¯Ú¯ÛŒ_Ø³Ú¯ÛŒ_Ø§Ø³Ø±Ø§Ø¦ÛŒÙ„ÛŒÙ‡Ø§']</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>akhonfellah</td>\n",
       "      <td>â¦ğŸ‡®ğŸ‡·â©Ø£Ø®ÙŒâ€ŒÙÙŠâ€ŒØ§Ù„Ù„Ù‡</td>\n",
       "      <td>Iran</td>\n",
       "      <td>en</td>\n",
       "      <td>fa</td>\n",
       "      <td>Ø­Ú©Ù…Øª Ø«Ø§Ø¨Øª Ù…ÙˆÙ†Ø¯Ù† Ø§Ø³Ù… Ù…Ø§Ù‡Ù‡Ø§ÛŒ Ù‚Ù…Ø±ÛŒ Ø¨Ø¹Ø¯ Ø§Ø² ØªØºÛŒÛŒØ± Ø²...</td>\n",
       "      <td>2017-11-19 19:40</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>akhonfellah</td>\n",
       "      <td>â¦ğŸ‡®ğŸ‡·â©Ø£Ø®ÙŒâ€ŒÙÙŠâ€ŒØ§Ù„Ù„Ù‡</td>\n",
       "      <td>Iran</td>\n",
       "      <td>en</td>\n",
       "      <td>fa</td>\n",
       "      <td>Ø¬Ù…Ù„Ù‡ Ø§ÛŒ Ú©Ù‡ Ø³ÛŒØ¯Ø­Ø³Ù† Ø§Ù…Ø´Ø¨ Ú¯ÙØª Ø¯Ø± Ù…ÙˆØ±Ø¯ Ø§Ù†Ù‚Ù„Ø§Ø¨ Ø§Ø³Ù„Ø§...</td>\n",
       "      <td>2019-02-06 18:26</td>\n",
       "      <td>[]</td>\n",
       "      <td>['Ø¥Ù†_Ù…Ø¹_Ø§Ù„ØµØ¨Ø±_Ù†ØµØ±Ø§']</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>akhonfellah</td>\n",
       "      <td>â¦ğŸ‡®ğŸ‡·â©Ø£Ø®ÙŒâ€ŒÙÙŠâ€ŒØ§Ù„Ù„Ù‡</td>\n",
       "      <td>Iran</td>\n",
       "      <td>en</td>\n",
       "      <td>fa</td>\n",
       "      <td>Ø¨Ø´Ù†ÙˆÛŒØ¯ Ù…Ø¯Ø­ Ø­Ø§Ø¬ Ù…Ø­Ù…ÙˆØ¯Ø¢Ù‚ÙˆÛŒ Ú©Ø±ÛŒÙ…ÛŒ Ø±Ùˆ Ø¨Ø§ Ù„Ù‡Ø¬Ù‡ Ø´ÛŒØ±Ø§...</td>\n",
       "      <td>2017-08-04 12:25</td>\n",
       "      <td>['https://twitter.com/khanisadiq/status/893438...</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>akhonfellah</td>\n",
       "      <td>â¦ğŸ‡®ğŸ‡·â©Ø£Ø®ÙŒâ€ŒÙÙŠâ€ŒØ§Ù„Ù„Ù‡</td>\n",
       "      <td>Iran</td>\n",
       "      <td>en</td>\n",
       "      <td>ar</td>\n",
       "      <td>RT @awadazeinab1: ÙƒÙ… Ø³Ø§Ø¹Ø© Ù…Ø¹ Ø§Ø¨Ù†ÙŠ Ø¨Ø§Ù„Ù…Ø³ØªØ´ÙÙ‰ Ø´Ù...</td>\n",
       "      <td>2019-01-25 17:48</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_screen_name user_display_name user_reported_location account_language  \\\n",
       "0      akhonfellah   â¦ğŸ‡®ğŸ‡·â©Ø£Ø®ÙŒâ€ŒÙÙŠâ€ŒØ§Ù„Ù„Ù‡                   Iran               en   \n",
       "1      akhonfellah   â¦ğŸ‡®ğŸ‡·â©Ø£Ø®ÙŒâ€ŒÙÙŠâ€ŒØ§Ù„Ù„Ù‡                   Iran               en   \n",
       "2      akhonfellah   â¦ğŸ‡®ğŸ‡·â©Ø£Ø®ÙŒâ€ŒÙÙŠâ€ŒØ§Ù„Ù„Ù‡                   Iran               en   \n",
       "3      akhonfellah   â¦ğŸ‡®ğŸ‡·â©Ø£Ø®ÙŒâ€ŒÙÙŠâ€ŒØ§Ù„Ù„Ù‡                   Iran               en   \n",
       "4      akhonfellah   â¦ğŸ‡®ğŸ‡·â©Ø£Ø®ÙŒâ€ŒÙÙŠâ€ŒØ§Ù„Ù„Ù‡                   Iran               en   \n",
       "5      akhonfellah   â¦ğŸ‡®ğŸ‡·â©Ø£Ø®ÙŒâ€ŒÙÙŠâ€ŒØ§Ù„Ù„Ù‡                   Iran               en   \n",
       "6      akhonfellah   â¦ğŸ‡®ğŸ‡·â©Ø£Ø®ÙŒâ€ŒÙÙŠâ€ŒØ§Ù„Ù„Ù‡                   Iran               en   \n",
       "7      akhonfellah   â¦ğŸ‡®ğŸ‡·â©Ø£Ø®ÙŒâ€ŒÙÙŠâ€ŒØ§Ù„Ù„Ù‡                   Iran               en   \n",
       "8      akhonfellah   â¦ğŸ‡®ğŸ‡·â©Ø£Ø®ÙŒâ€ŒÙÙŠâ€ŒØ§Ù„Ù„Ù‡                   Iran               en   \n",
       "9      akhonfellah   â¦ğŸ‡®ğŸ‡·â©Ø£Ø®ÙŒâ€ŒÙÙŠâ€ŒØ§Ù„Ù„Ù‡                   Iran               en   \n",
       "\n",
       "  tweet_language                                         tweet_text  \\\n",
       "0             en  one person followed me // automatically checke...   \n",
       "1             fa  #IDFTerrorists\\nØ­Ù…Ø§Ø³Ù‡ ØªØ±ÙˆØ±ÛŒØ³ØªÙ‡Ø§ÛŒ Ø§Ø³Ø±Ø§Ø¦ÛŒÙ„ÛŒ http...   \n",
       "2             en    Stop war on Yemen hospitals\\n#ShameOnUN\\n#Yemen   \n",
       "3             fa  Ù„Ø¨ÛŒÚ© ÛŒØ§ ÙÙ‚ÛŒÙ‡\\n#Ù…Ø¬Ø²Ø±Ø©_Ø§Ù„Ø¯Ø±Ø§Ø² https://t.co/nKfQW...   \n",
       "4             fa  Ø§ÛŒÙ†Ø¬Ø§ ØªÙ„ Ø§Ø¨ÛŒØ¨ Ø§Ø³Øª\\nØ§ÛŒÙ†Ù‡Ø§ Ø§Ø³Ø±Ø§ÛŒÛŒÙ„ÛŒÙ‡Ø§ÛŒÛŒ Ù‡Ø³ØªÙ†Ø¯ Ú©Ù‡...   \n",
       "5             ar  ÙˆØ§Ù…Ø±ÙˆØ² Ù‡Ù… ....\\n#Ø²Ù†Ø¯Ú¯ÛŒ_Ø³Ú¯ÛŒ_Ø§Ø³Ø±Ø§Ø¦ÛŒÙ„ÛŒÙ‡Ø§ https://...   \n",
       "6             fa  Ø­Ú©Ù…Øª Ø«Ø§Ø¨Øª Ù…ÙˆÙ†Ø¯Ù† Ø§Ø³Ù… Ù…Ø§Ù‡Ù‡Ø§ÛŒ Ù‚Ù…Ø±ÛŒ Ø¨Ø¹Ø¯ Ø§Ø² ØªØºÛŒÛŒØ± Ø²...   \n",
       "7             fa  Ø¬Ù…Ù„Ù‡ Ø§ÛŒ Ú©Ù‡ Ø³ÛŒØ¯Ø­Ø³Ù† Ø§Ù…Ø´Ø¨ Ú¯ÙØª Ø¯Ø± Ù…ÙˆØ±Ø¯ Ø§Ù†Ù‚Ù„Ø§Ø¨ Ø§Ø³Ù„Ø§...   \n",
       "8             fa  Ø¨Ø´Ù†ÙˆÛŒØ¯ Ù…Ø¯Ø­ Ø­Ø§Ø¬ Ù…Ø­Ù…ÙˆØ¯Ø¢Ù‚ÙˆÛŒ Ú©Ø±ÛŒÙ…ÛŒ Ø±Ùˆ Ø¨Ø§ Ù„Ù‡Ø¬Ù‡ Ø´ÛŒØ±Ø§...   \n",
       "9             ar  RT @awadazeinab1: ÙƒÙ… Ø³Ø§Ø¹Ø© Ù…Ø¹ Ø§Ø¨Ù†ÙŠ Ø¨Ø§Ù„Ù…Ø³ØªØ´ÙÙ‰ Ø´Ù...   \n",
       "\n",
       "         tweet_time                                               urls  \\\n",
       "0  2017-01-11 05:23                              ['http://fllwrs.com']   \n",
       "1  2018-05-26 00:48                                                 []   \n",
       "2  2018-06-16 20:06                                                 []   \n",
       "3  2018-05-23 18:22                                                 []   \n",
       "4  2019-01-28 16:56                                                 []   \n",
       "5  2018-09-07 10:42                                                 []   \n",
       "6  2017-11-19 19:40                                                 []   \n",
       "7  2019-02-06 18:26                                                 []   \n",
       "8  2017-08-04 12:25  ['https://twitter.com/khanisadiq/status/893438...   \n",
       "9  2019-01-25 17:48                                                 []   \n",
       "\n",
       "                   hashtags  is_retweet  \n",
       "0                        []       False  \n",
       "1         ['IDFTerrorists']       False  \n",
       "2    ['ShameOnUN', 'Yemen']       False  \n",
       "3          ['Ù…Ø¬Ø²Ø±Ø©_Ø§Ù„Ø¯Ø±Ø§Ø²']       False  \n",
       "4  ['Ø²Ù†Ø¯Ú¯ÛŒ_Ø³Ú¯ÛŒ_Ø§Ø³Ø±Ø§Ø¦ÛŒÙ„ÛŒÙ‡Ø§']       False  \n",
       "5  ['Ø²Ù†Ø¯Ú¯ÛŒ_Ø³Ú¯ÛŒ_Ø§Ø³Ø±Ø§Ø¦ÛŒÙ„ÛŒÙ‡Ø§']       False  \n",
       "6                        []       False  \n",
       "7      ['Ø¥Ù†_Ù…Ø¹_Ø§Ù„ØµØ¨Ø±_Ù†ØµØ±Ø§']       False  \n",
       "8                        []       False  \n",
       "9                        []        True  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_clean = tweets[['user_screen_name',  'user_display_name', 'user_reported_location', 'account_language', 'tweet_language', 'tweet_text', 'tweet_time', 'urls', 'hashtags', 'is_retweet']]\n",
    "\n",
    "# Top 10 rows of data\n",
    "tweets_clean.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c.) Filter Tweets and keeps those that are either \n",
    " - account located in Venezuela\n",
    " - account language Spanish OR\n",
    " - tweet language Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_clean = tweets_clean[(tweets_clean.user_reported_location == 'Venezuela') | (tweets_clean.account_language == 'es') | (tweets_clean.tweet_language == 'es')].reset_index().drop(columns= ['index'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d.) Take out Tweets that are set in European and US location "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_screen_name</th>\n",
       "      <th>user_display_name</th>\n",
       "      <th>user_reported_location</th>\n",
       "      <th>account_language</th>\n",
       "      <th>tweet_language</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>tweet_time</th>\n",
       "      <th>urls</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>is_retweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>akhonfellah</td>\n",
       "      <td>â¦ğŸ‡®ğŸ‡·â©Ø£Ø®ÙŒâ€ŒÙÙŠâ€ŒØ§Ù„Ù„Ù‡</td>\n",
       "      <td>Iran</td>\n",
       "      <td>en</td>\n",
       "      <td>es</td>\n",
       "      <td>No parallel in history....\\n#Hussain https://t...</td>\n",
       "      <td>2018-09-19 20:04</td>\n",
       "      <td>[]</td>\n",
       "      <td>['Hussain']</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>akhonfellah</td>\n",
       "      <td>â¦ğŸ‡®ğŸ‡·â©Ø£Ø®ÙŒâ€ŒÙÙŠâ€ŒØ§Ù„Ù„Ù‡</td>\n",
       "      <td>Iran</td>\n",
       "      <td>en</td>\n",
       "      <td>es</td>\n",
       "      <td>HABIL CAFEğŸ˜‚ https://t.co/5ipZYvYA8X</td>\n",
       "      <td>2017-10-04 03:28</td>\n",
       "      <td>['https://twitter.com/KnowKaduna/status/915295...</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Romeo1997er</td>\n",
       "      <td>Romeo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fa</td>\n",
       "      <td>es</td>\n",
       "      <td>RT @countdown2040: Gazans prepare for Hajj\\nht...</td>\n",
       "      <td>2017-08-29 03:17</td>\n",
       "      <td>['http://www.countdown2040.com/ShowGallery/69/']</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kasia36790875</td>\n",
       "      <td>Kasia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fa</td>\n",
       "      <td>es</td>\n",
       "      <td>RT @countdown2040: Abbas sends medical aid to ...</td>\n",
       "      <td>2017-11-18 16:36</td>\n",
       "      <td>['http://www.countdown2040.com/ShowNews/1014/']</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hmn90432381</td>\n",
       "      <td>H.m.n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fa</td>\n",
       "      <td>es</td>\n",
       "      <td>RT @countdown2040: Gazans prepare for Hajj\\nht...</td>\n",
       "      <td>2017-09-24 14:28</td>\n",
       "      <td>['http://www.countdown2040.com/ShowGallery/69/']</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>koInQlW0rKxQPoTuf5BmVjKyTvYJR5JdKeo8spDdrwM=</td>\n",
       "      <td>koInQlW0rKxQPoTuf5BmVjKyTvYJR5JdKeo8spDdrwM=</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fa</td>\n",
       "      <td>es</td>\n",
       "      <td>RT @countdown2040: Abbas sends medical aid to ...</td>\n",
       "      <td>2017-10-30 16:52</td>\n",
       "      <td>['http://www.countdown2040.com/ShowNews/1014/']</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>24evm+SfMta5ONKMRjQe1Qj39PdyLGPqMMXl8XYDbg=</td>\n",
       "      <td>24evm+SfMta5ONKMRjQe1Qj39PdyLGPqMMXl8XYDbg=</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>es</td>\n",
       "      <td>RT @countdown2040: Video: Lana Del Rey Ignores...</td>\n",
       "      <td>2018-08-28 08:33</td>\n",
       "      <td>['http://www.countdown2040.com/ShowMovieList/9...</td>\n",
       "      <td>['GroupPalestine', 'Ù‚Ø±ÙˆØ¨_ÙÙ„Ø³Ø·ÙŠÙ†ÙŠ']</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>GP3PzukVFPWVoLtVWTEyy20m2lRZaMaRmN7n0lz7Bg=</td>\n",
       "      <td>GP3PzukVFPWVoLtVWTEyy20m2lRZaMaRmN7n0lz7Bg=</td>\n",
       "      <td>Earth</td>\n",
       "      <td>fa</td>\n",
       "      <td>es</td>\n",
       "      <td>RT @countdown2040: Gazans prepare for Hajj\\nht...</td>\n",
       "      <td>2017-09-28 17:53</td>\n",
       "      <td>['http://www.countdown2040.com/ShowGallery/69/']</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Richard80907</td>\n",
       "      <td>Richard</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fa</td>\n",
       "      <td>es</td>\n",
       "      <td>RT @countdown2040: Gazans prepare for Hajj\\nht...</td>\n",
       "      <td>2017-11-05 15:47</td>\n",
       "      <td>['http://www.countdown2040.com/ShowGallery/69/']</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>UW2EZRTAv0C7rCy2LOI2SBiYh8IrdwmQAdI7p7yqok=</td>\n",
       "      <td>UW2EZRTAv0C7rCy2LOI2SBiYh8IrdwmQAdI7p7yqok=</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>es</td>\n",
       "      <td>Abbas sends medical aid to Venezuela, sparking...</td>\n",
       "      <td>2017-08-23 08:02</td>\n",
       "      <td>['http://www.UW2EZRTAv0C7rCy2LOI2SBiYh8IrdwmQA...</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               user_screen_name  \\\n",
       "0                                   akhonfellah   \n",
       "1                                   akhonfellah   \n",
       "2                                   Romeo1997er   \n",
       "3                                 Kasia36790875   \n",
       "4                                   Hmn90432381   \n",
       "5  koInQlW0rKxQPoTuf5BmVjKyTvYJR5JdKeo8spDdrwM=   \n",
       "6   24evm+SfMta5ONKMRjQe1Qj39PdyLGPqMMXl8XYDbg=   \n",
       "7   GP3PzukVFPWVoLtVWTEyy20m2lRZaMaRmN7n0lz7Bg=   \n",
       "8                                  Richard80907   \n",
       "9   UW2EZRTAv0C7rCy2LOI2SBiYh8IrdwmQAdI7p7yqok=   \n",
       "\n",
       "                              user_display_name user_reported_location  \\\n",
       "0                               â¦ğŸ‡®ğŸ‡·â©Ø£Ø®ÙŒâ€ŒÙÙŠâ€ŒØ§Ù„Ù„Ù‡                   Iran   \n",
       "1                               â¦ğŸ‡®ğŸ‡·â©Ø£Ø®ÙŒâ€ŒÙÙŠâ€ŒØ§Ù„Ù„Ù‡                   Iran   \n",
       "2                                         Romeo                    NaN   \n",
       "3                                         Kasia                    NaN   \n",
       "4                                         H.m.n                    NaN   \n",
       "5  koInQlW0rKxQPoTuf5BmVjKyTvYJR5JdKeo8spDdrwM=                    NaN   \n",
       "6   24evm+SfMta5ONKMRjQe1Qj39PdyLGPqMMXl8XYDbg=                    NaN   \n",
       "7   GP3PzukVFPWVoLtVWTEyy20m2lRZaMaRmN7n0lz7Bg=                 Earth    \n",
       "8                                       Richard                    NaN   \n",
       "9   UW2EZRTAv0C7rCy2LOI2SBiYh8IrdwmQAdI7p7yqok=                    NaN   \n",
       "\n",
       "  account_language tweet_language  \\\n",
       "0               en             es   \n",
       "1               en             es   \n",
       "2               fa             es   \n",
       "3               fa             es   \n",
       "4               fa             es   \n",
       "5               fa             es   \n",
       "6               en             es   \n",
       "7               fa             es   \n",
       "8               fa             es   \n",
       "9               en             es   \n",
       "\n",
       "                                          tweet_text        tweet_time  \\\n",
       "0  No parallel in history....\\n#Hussain https://t...  2018-09-19 20:04   \n",
       "1                HABIL CAFEğŸ˜‚ https://t.co/5ipZYvYA8X  2017-10-04 03:28   \n",
       "2  RT @countdown2040: Gazans prepare for Hajj\\nht...  2017-08-29 03:17   \n",
       "3  RT @countdown2040: Abbas sends medical aid to ...  2017-11-18 16:36   \n",
       "4  RT @countdown2040: Gazans prepare for Hajj\\nht...  2017-09-24 14:28   \n",
       "5  RT @countdown2040: Abbas sends medical aid to ...  2017-10-30 16:52   \n",
       "6  RT @countdown2040: Video: Lana Del Rey Ignores...  2018-08-28 08:33   \n",
       "7  RT @countdown2040: Gazans prepare for Hajj\\nht...  2017-09-28 17:53   \n",
       "8  RT @countdown2040: Gazans prepare for Hajj\\nht...  2017-11-05 15:47   \n",
       "9  Abbas sends medical aid to Venezuela, sparking...  2017-08-23 08:02   \n",
       "\n",
       "                                                urls  \\\n",
       "0                                                 []   \n",
       "1  ['https://twitter.com/KnowKaduna/status/915295...   \n",
       "2   ['http://www.countdown2040.com/ShowGallery/69/']   \n",
       "3    ['http://www.countdown2040.com/ShowNews/1014/']   \n",
       "4   ['http://www.countdown2040.com/ShowGallery/69/']   \n",
       "5    ['http://www.countdown2040.com/ShowNews/1014/']   \n",
       "6  ['http://www.countdown2040.com/ShowMovieList/9...   \n",
       "7   ['http://www.countdown2040.com/ShowGallery/69/']   \n",
       "8   ['http://www.countdown2040.com/ShowGallery/69/']   \n",
       "9  ['http://www.UW2EZRTAv0C7rCy2LOI2SBiYh8IrdwmQA...   \n",
       "\n",
       "                             hashtags  is_retweet  \n",
       "0                         ['Hussain']       False  \n",
       "1                                  []       False  \n",
       "2                                  []        True  \n",
       "3                                  []        True  \n",
       "4                                  []        True  \n",
       "5                                  []        True  \n",
       "6  ['GroupPalestine', 'Ù‚Ø±ÙˆØ¨_ÙÙ„Ø³Ø·ÙŠÙ†ÙŠ']        True  \n",
       "7                                  []        True  \n",
       "8                                  []        True  \n",
       "9                                  []       False  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_clean = tweets_clean[(tweets_clean.user_reported_location != 'London') & (tweets_clean.user_reported_location != 'Manhattan, NY') & (tweets_clean.user_reported_location != 'Brooklyn, NY') & (tweets_clean.user_reported_location != 'Queens, NY') & (tweets_clean.user_reported_location != 'New York, NY') & (tweets_clean.user_reported_location != 'California, USA') & (tweets_clean.user_reported_location != 'New Jersey, USA') &  (tweets_clean.user_reported_location != 'North Holland, The Netherlands') & (tweets_clean.user_reported_location != 'Atlantic City, NJ') & (tweets_clean.user_reported_location != 'Mountain View, CA') & (tweets_clean.user_reported_location != 'New York, USA') & (tweets_clean.user_reported_location != 'Canada') & (tweets_clean.user_reported_location != 'San Francisco, CA') & (tweets_clean.user_reported_location != 'Washington, USA') & (tweets_clean.user_reported_location != 'Washington, DC') & (tweets_clean.user_reported_location != 'EspaÃ±a') & (tweets_clean.user_reported_location != 'Germany') & (tweets_clean.user_reported_location != 'Nantes, France') & (tweets_clean.user_reported_location != 'Houston, TX') & (tweets_clean.user_reported_location != 'Texas,San Antonio') & (tweets_clean.user_reported_location != 'Chicago') & (tweets_clean.user_reported_location != 'Atlanta') & (tweets_clean.user_reported_location != 'Washington,Seattle') & (tweets_clean.user_reported_location != 'Fremont, CA') & (tweets_clean.user_reported_location != 'France') & (tweets_clean.user_reported_location != 'England, United Kingdom')  & (tweets_clean.user_reported_location != 'Oregon,Portland')  & (tweets_clean.user_reported_location !='USA')  & (tweets_clean.user_reported_location != 'Florida,Orlando') & (tweets_clean.user_reported_location != 'Califor') & (tweets_clean.user_reported_location !='California,Los Angeles') & (tweets_clean.user_reported_location !='Illinois, USA') & (tweets_clean.user_reported_location !='Arizona,phoenix') & (tweets_clean.user_reported_location !='Pennsylvania,Pittsburgh') & (tweets_clean.user_reported_location !='Pennsylvania,Philadelphia') & (tweets_clean.user_reported_location !='Dallas, TX') ]\n",
    "\n",
    "# Top 10 rows of data\n",
    "tweets_clean.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e.) Quality control for Twitter language identification \n",
    "- Import SpaCy, a text-processing package, and load Spanish and English models and Language Identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy_langdetect import LanguageDetector\n",
    "\n",
    "# Initialize spacy with the SPANISH model\n",
    "sp = spacy.load('es_core_news_sm')\n",
    "sp.add_pipe(LanguageDetector(), name = 'language_detector', last = True)\n",
    "eng = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Identify language of each Tweet using SpaCy language identification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_language(df, content_column):\n",
    "    '''\n",
    "    A function to detect the language in each tweet and add to new row\n",
    "\n",
    "    Argument: a dataframe  and content column\n",
    "    Ouput: same dataframe with a new 'cleaned_content' column\n",
    "    '''\n",
    "\n",
    "    # Initialize list of languages\n",
    "    spacy_language_detection = []\n",
    "\n",
    "    # Call detect the language for each row in the data frame and append to spacy_language_detection list\n",
    "    for row in df[content_column]:\n",
    "        doc = sp(row)\n",
    "        spacy_language_detection.append(doc._.language['language'])\n",
    "\n",
    "    # Append language list to the data frame\n",
    "    df['spacy_language_detection'] = spacy_language_detection\n",
    "\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_clean = detect_language(df = tweets_clean, content_column = 'tweet_text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Keep Tweets that are marked as either Spanish by BOTH Twitter and SpaCy or English by BOTH Twitter and SpaCy.\n",
    "- Separate into 'Spanish' and 'English' datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_tweets = tweets_clean[(tweets_clean.tweet_language == 'es') & (tweets_clean.spacy_language_detection == 'es')]\n",
    "english_tweets = tweets_clean[(tweets_clean.tweet_language == 'en') & (tweets_clean.spacy_language_detection == 'en')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Keyword analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a.) Use SpaCy to â€œcleanâ€ each Tweet by removing stop words (very common words that carry little meaning), non alpha-numeric characters, and by reducing each word to itâ€™s lemma or root. \n",
    "This will allow us to create a consistent vocabulary to analyze keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(text_string, language):\n",
    "    '''\n",
    "    A function to clean a string using SpaCy, removing stop-words and non-alphanumeric characters\n",
    "\n",
    "    Argument: a text string and a language ('English' or 'Spanish')\n",
    "    Output: a cleaned string\n",
    "\n",
    "    '''\n",
    "    if language == 'Spanish':\n",
    "    # Parse the text string using the english model initialized earlier\n",
    "        doc = sp(text_string)\n",
    "    elif language == 'English':\n",
    "        doc = eng(text_string)\n",
    "    \n",
    "    # Initialize empty string\n",
    "    clean = []\n",
    "\n",
    "    # Add each token to the list if it is not a stop word, is alphanumeric, and if it's not a pronoun\n",
    "    for token in doc:\n",
    "        \n",
    "        if token.is_alpha == False or token.is_stop == True:\n",
    "            pass\n",
    "        else:\n",
    "            clean.append(token.lemma_)\n",
    "\n",
    "    # Join the list into a string\n",
    "    clean = \" \".join(clean)\n",
    "\n",
    "    return clean\n",
    "\n",
    "def clean_content(df, content_column, language):\n",
    "    '''\n",
    "    A function to clean all the strings in a whole of a corpus\n",
    "\n",
    "    Argument: a dataframe, the name of the column with the content, and a language ('Spanish' or 'English')\n",
    "    Ouput: same dataframe with a new cleaned content column\n",
    "    '''\n",
    "\n",
    "    # Initialize list of cleaned content strings\n",
    "    clean_content= []\n",
    "\n",
    "    # Call clean_string() for each row in the data frame and append to clean_content list\n",
    "    for row in df[content_column]:\n",
    "        clean_content.append(clean_string(row, language))\n",
    "\n",
    "    # Append clean_content list to the data frame\n",
    "    df['lemmatized_tweet_text'] = clean_content\n",
    "\n",
    "    return df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Here is an example of how this works***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Spanish example: \n",
      "RT @MFloresBazaldua: lo que no ves con tus ojos, no lo inventes con tu boca http://t.co/oOzAorw7Fc\n",
      "\n",
      "Clean exmaple: \n",
      "RT ver ojo inventar boca\n",
      "\n",
      "\n",
      "\n",
      "Raw English example: \n",
      "Great tool. Very easy to use and it does the job very well! I highly recommend it! #BulkFollower https://t.co/SzGtLXW16B\n",
      "\n",
      "Clean exmaple: \n",
      "great tool very easy use job -PRON- highly recommend bulkfollower\n"
     ]
    }
   ],
   "source": [
    "example_sp = spanish_tweets.loc[:,'tweet_text'][0]\n",
    "example_sp_clean = clean_string(example_sp, 'Spanish')\n",
    "print(\"Raw Spanish example: \\n\" + example_sp)\n",
    "print(\"\\nClean exmaple: \\n\" + example_sp_clean)\n",
    "\n",
    "\n",
    "example_eng = english_tweets.loc[:,'tweet_text'][4]\n",
    "example_eng_clean = clean_string(example_eng, \"English\")\n",
    "print(\"\\n\\n\\nRaw English example: \\n\" + example_eng)\n",
    "print(\"\\nClean exmaple: \\n\" + example_eng_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_tweets = pd.read_csv('xlsx/spanish_tweets.csv', encoding='utf-8-sig', index_col = 0).reset_index().drop(columns='index')\n",
    "english_tweets = pd.read_csv('xlsx/english_tweets.csv', encoding='utf-8-sig', index_col = 0).reset_index().drop(columns='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b.) Use Scikit-learn â€˜CountVectorizerâ€™ package to create word counts for Spanish and English tweets\n",
    "First we create a matrix where each column is a word found in any Tweet and each row is the number of times in occurs in each Tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# only words that appear in more than 5 tweets\n",
    "word_vectorizer = CountVectorizer(encoding='utf-8-sig', analyzer='word', min_df=5, ngram_range=(1,1))\n",
    "word_count_sm = word_vectorizer.fit_transform(spanish_tweets['lemmatized_tweet_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we sum the count of each word over all Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_vectorizer.get_feature_names()\n",
    "word_count_total = word_count_sm.sum(axis=0)\n",
    "word_count_total_df = pd.DataFrame(word_count_total, columns = words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b.) Use Scikit-learn â€˜CountVectorizerâ€™ package to create phrase counts for Spanish and English tweets\n",
    "We set the length of phrases to be between 3 and 5 words long for one data frame and between 5 to 7 for the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_vectorizer = CountVectorizer(encoding='utf-8-sig', analyzer='word', min_df=5, ngram_range=(5,7))\n",
    "# create matrix where each column is a phrase and each row is a count in each tweet\n",
    "phrase_count_sm = phrase_vectorizer.fit_transform(spanish_tweets['lemmatized_tweet_text'])\n",
    "phrases = phrase_vectorizer.get_feature_names()\n",
    "# sum the count of each phrase over all Tweets\n",
    "phrase_count_total = phrase_count_sm.sum(axis=0)\n",
    "phrase_count_total_df = pd.DataFrame(phrase_count_total, columns = phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
